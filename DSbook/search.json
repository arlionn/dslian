[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "数据分析",
    "section": "",
    "text": "连享会主页\n课程\n视频\n资料\n\n\n\n主页 || 课程 || 视频 || 推文 || 资料",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>数据分析</span>"
    ]
  },
  {
    "objectID": "body/_home.html",
    "href": "body/_home.html",
    "title": "About",
    "section": "",
    "text": "连享会小课堂：在线视频课程\n连享会 由中山大学连玉君老师团队创办，定期分享各类实证分析经验，主要栏目如下：",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>About</span>"
    ]
  },
  {
    "objectID": "body/_home.html#连享会小课堂在线视频课程",
    "href": "body/_home.html#连享会小课堂在线视频课程",
    "title": "About",
    "section": "",
    "text": "\\({\\color{red}{NEW}}\\) 连享会在线课堂：lianxh-class.cn\n\n\n连玉君，Stata 33 讲，观看量超过 100 万人次的 Stata 入门课\n连玉君，直击面板数据模型，2 小时，公开课\nStata 软件及计量基础, 五次课，Stata 入门必备\n因果推断：控制变量如何选？，2 小时, 9.9 元\n因果推断：反事实架构及主流计量方法, 2 小时, 9.9 元\n我的特斯拉-实证研究设计，2.4 小时\n连玉君，我的甲壳虫-论文精讲与重现，两次课，共 6 小时\n连玉君，动态面板数据模型，2.2 小时，理论+实操 ……",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>About</span>"
    ]
  },
  {
    "objectID": "body/_home.html#资源分享",
    "href": "body/_home.html#资源分享",
    "title": "About",
    "section": "资源分享",
    "text": "资源分享\n\n详情：主页 - 资料分享\n\n\nData\n\nCSMAR-国泰安 | Wind-万德 | Resset-锐思\n常用数据库 | 人文社科数据库\n\n\n\n论文复现\n\n论文重现网站大全\n\n\nDiscover Mendeley Data\nICPSR Data\nHarvard Dataverse\nFind Economic Articles with Data\n\n包含 9000 多篇经济金融论文，可检索软件类型、期刊名称等。\n\nReplication in the social sciences\n\n\n\n推文\n\nStata 教程 | 资料 | 新命令 | 结果输出 | 绘图 | 数据处理 | 程序\n回归分析 | 面板数据 | 交乘项-调节 | IV-GMM | Logit | 空间计量\n因果推断 | DID | RDD | PSM | 合成控制 | 文本分析\nMarkdown | 工具软件 | 机器学习 | 其它\nPDF 合集",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>About</span>"
    ]
  },
  {
    "objectID": "body/_home.html#联系我们",
    "href": "body/_home.html#联系我们",
    "title": "About",
    "section": "联系我们",
    "text": "联系我们\n\nE-mail： StataChina@163.com\n微信公众号： lianxh_cn (连享会)\n\n\n数字连享会\n截至 2024.12.18，连享会 共发布推文 1351 篇，累积阅读量 13,043,554 次。篇均阅读次数为 9654。连享会 知乎账号 回答了 1360 个经济、管理领域的问题，发布推文 1036 篇，累积阅读量为 58,564,198 次。\n\n\n欢迎赐稿\n\n推文风格：参见 分类推文。\n录用 2 篇 以上，即可 免费 获得一期 Stata 现场培训资格\n投稿信箱: StataChina@163.com / arlionn@163.com 。\n邮件标题：推文投稿-姓名：推文标题。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>About</span>"
    ]
  },
  {
    "objectID": "body/C00-为何学习Python？.html",
    "href": "body/C00-为何学习Python？.html",
    "title": "Ch0.0 为什么要学 Python？",
    "section": "",
    "text": "目标\n本章将为你解释为什么学习 Python 对于金融与经济决策领域的重要性。我们将探索 Python 在数据处理、建模、自动化、预测和报告生成等方面的应用场景，帮助你理解 Python 的通用性与开放生态。",
    "crumbs": [
      "**前言**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ch0.0 为什么要学 Python？</span>"
    ]
  },
  {
    "objectID": "body/C00-为何学习Python？.html#python-在金融经济决策中的应用场景",
    "href": "body/C00-为何学习Python？.html#python-在金融经济决策中的应用场景",
    "title": "Ch0.0 为什么要学 Python？",
    "section": "1. Python 在金融/经济决策中的应用场景",
    "text": "1. Python 在金融/经济决策中的应用场景\n\n1.1 数据处理\n在金融与经济学研究中，我们处理的数据量往往庞大且复杂，数据来源多样（如：Excel、CSV、数据库、API 等）。Python 是一个非常强大的数据处理工具，能够帮助我们高效地清理、转换、汇总和分析数据。\n常见应用场景包括：\n\n数据清洗：处理缺失值、异常值、重复数据、格式化数据等。\n数据转换：通过 pandas 对数据进行筛选、合并、拆分等操作。\n数据汇总与聚合：根据不同的维度进行数据汇总，计算统计量（均值、标准差等）。\n\n\n示例\nimport pandas as pd\n\n# 导入数据\ndata = pd.read_csv('financial_data.csv')\n\n# 清理缺失值\ndata = data.dropna()\n\n# 按行业汇总平均收入\nsummary = data.groupby('industry')['income'].mean()\nprint(summary)\n\n\n\n1.2 建模\n在经济学和金融研究中，我们常常需要构建统计模型，分析变量之间的关系，预测未来的走势。Python 提供了丰富的统计建模库，如 statsmodels 和 scikit-learn，这些库支持回归分析、分类模型、时间序列分析等。\n常见的建模任务包括：\n\n回归分析：例如，分析广告投入对销售额的影响。\n分类任务：例如，预测客户是否会违约。\n时间序列预测：例如，预测股票价格、利率等。\n\n\n示例\nfrom sklearn.linear_model import LinearRegression\n\n# 构建回归模型\nmodel = LinearRegression()\nX = data[['ad_spend', 'market_size']]  # 自变量\ny = data['sales']  # 因变量\nmodel.fit(X, y)\n\n# 输出回归系数\nprint(model.coef_)\n\n\n\n1.3 自动化\nPython 强大的脚本编写能力使得金融与经济分析的自动化成为可能。你可以编写脚本来自动化常见的任务，如定期数据更新、报告生成、图表绘制等。通过自动化，我们能够减少手动操作的错误和时间浪费，提高工作效率。\n常见的自动化任务包括：\n\n定时抓取金融数据：如自动从 Yahoo Finance 或 Bloomberg 获取股票数据。\n自动生成报告：根据分析结果生成 Excel 或 PDF 报告。\n自动化交易：通过算法策略自动进行股票或期货交易。\n\n\n示例\nimport yfinance as yf\n\n# 自动下载特定股票的历史数据\nstock_data = yf.download('AAPL', start='2020-01-01', end='2021-01-01')\n\n# 保存数据为 CSV 文件\nstock_data.to_csv('apple_stock_data.csv')\n\n\n\n1.4 预测\n在金融与经济领域，预测是一个非常重要的任务，例如股市走势预测、宏观经济指标预测等。Python 提供了许多机器学习和统计学方法，可以帮助我们建立高效的预测模型。通过训练模型，我们可以对未来的变化进行预测，帮助决策者制定相应的策略。\n常见的预测任务包括：\n\n股票价格预测：使用历史数据预测股票的未来走势。\n经济指标预测：例如，预测 GDP 增长率、通货膨胀率等。\n客户需求预测：例如，预测未来的客户购买行为。\n\n\n示例\nfrom sklearn.ensemble import RandomForestRegressor\n\n# 构建预测模型\nrf_model = RandomForestRegressor(n_estimators=100)\nX = data[['ad_spend', 'market_size']]\ny = data['sales']\nrf_model.fit(X, y)\n\n# 预测未来的销售额\npredictions = rf_model.predict([[50000, 1000]])  # 给定广告花费与市场规模\nprint(predictions)\n\n\n\n1.5 报告生成\n金融分析报告是金融决策中的核心组成部分。使用 Python，你可以自动化地生成图表、表格和文本报告，并将其输出为 PDF、Word 或 HTML 格式。通过编写报告生成脚本，分析师可以快速、准确地生成报告，并减少手动编写过程中的出错率。\n常见应用场景包括：\n\n自动生成图表：如利润趋势、资产负债表等。\n定期报告：每月、每季度生成财务报表。\n报告内容自动填充：根据数据结果填充报告模板。\n\n\n示例\nimport matplotlib.pyplot as plt\n\n# 绘制销售趋势图\nplt.plot(data['date'], data['sales'])\nplt.title('Sales Trend')\nplt.xlabel('Date')\nplt.ylabel('Sales')\nplt.savefig('sales_trend.png')",
    "crumbs": [
      "**前言**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ch0.0 为什么要学 Python？</span>"
    ]
  },
  {
    "objectID": "body/C00-为何学习Python？.html#python-的通用性与开放生态",
    "href": "body/C00-为何学习Python？.html#python-的通用性与开放生态",
    "title": "Ch0.0 为什么要学 Python？",
    "section": "2. Python 的通用性与开放生态",
    "text": "2. Python 的通用性与开放生态\n\n2.1 通用性\nPython 是一门通用编程语言，适用于各种领域，不仅限于数据科学和金融分析。无论是 Web 开发、自动化任务、游戏开发还是机器学习，Python 都能胜任。因此，学习 Python 后，你可以在不同领域间灵活转型，具备广泛的就业竞争力。\n\n\n2.2 开放生态\nPython 拥有非常庞大的开发者社区，几乎每个可能的应用场景都有现成的开源库。例如，金融领域中有专门的 QuantLib，机器学习领域有 scikit-learn 和 TensorFlow，自然语言处理领域有 spaCy 和 NLTK。这些库大多免费开源，且文档丰富，易于上手。\n\n\n2.3 与其他工具的兼容性\nPython 可以无缝与其他工具（如 Excel、Stata、R 等）协作。你可以将 Python 生成的数据输出导入 Excel 中，或者通过 R 与 Python 的接口（如 rpy2）实现两者的互通。此外，Python 还能够通过 API 与第三方平台（如金融数据供应商、银行系统等）进行连接，获取实时数据。",
    "crumbs": [
      "**前言**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ch0.0 为什么要学 Python？</span>"
    ]
  },
  {
    "objectID": "body/C00-为何学习Python？.html#总结",
    "href": "body/C00-为何学习Python？.html#总结",
    "title": "Ch0.0 为什么要学 Python？",
    "section": "总结",
    "text": "总结\n通过本章的介绍，你可以看到 Python 在金融与经济决策中的广泛应用场景，它能够帮助我们更高效地处理数据、进行建模、自动化任务、进行预测并生成报告。Python 不仅仅是一个编程语言，更是一个开放且强大的生态系统，它为金融与经济决策提供了无数的可能性。\n\n提示： 如果你对 Python 感兴趣，建议你在学习过程中通过实践不断巩固和应用知识，尝试解决实际问题，逐步熟悉其强大的数据处理与建模能力。\n\n\n\n\n\n\n\n测试提示框\n\n\n\n\\[\ny_{it} = x_{it}\\beta + \\varepsilon_{it}\n\\]",
    "crumbs": [
      "**前言**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ch0.0 为什么要学 Python？</span>"
    ]
  },
  {
    "objectID": "body/C00-为何学习Python？.html#python-资源",
    "href": "body/C00-为何学习Python？.html#python-资源",
    "title": "Ch0.0 为什么要学 Python？",
    "section": "Python 资源",
    "text": "Python 资源\n\nGithub 仓库\n\n&lt;https://github.com/ml-tooling/best-of-ml-python&gt;",
    "crumbs": [
      "**前言**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ch0.0 为什么要学 Python？</span>"
    ]
  },
  {
    "objectID": "body/print_02_format.html",
    "href": "body/print_02_format.html",
    "title": "格式化字符串：f-string 格式",
    "section": "",
    "text": "1. f-string 语法回顾\nPython 格式化字符串进阶：f-string 格式控制符一览，轻松掌握 .2f, :,, &gt;, ^, % 的使用方法。\n参见\n文本序列类型 — str\n字符串是 序列类型 ，支持序列类型的各种操作。\n字符串的方法\n字符串支持很多变形与查找方法。\nf 字符串\n内嵌表达式的字符串字面值。\n格式字符串语法\n使用 str.format() 格式化字符串。\nprintf 风格的字符串格式化\n这里详述了用 % 运算符格式化字符串的操作。\n基本语法形式为：\n其中 : 后面的 格式说明 部分就是 格式控制符（Format Specifier），用于控制数值的显示样式。",
    "crumbs": [
      "语法格式",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>格式化字符串：f-string 格式</span>"
    ]
  },
  {
    "objectID": "body/print_02_format.html#f-string-语法回顾",
    "href": "body/print_02_format.html#f-string-语法回顾",
    "title": "格式化字符串：f-string 格式",
    "section": "",
    "text": "f\"{变量名:格式说明}\"",
    "crumbs": [
      "语法格式",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>格式化字符串：f-string 格式</span>"
    ]
  },
  {
    "objectID": "body/print_02_format.html#数值格式控制符对照表",
    "href": "body/print_02_format.html#数值格式控制符对照表",
    "title": "格式化字符串：f-string 格式",
    "section": "2. 数值格式控制符对照表",
    "text": "2. 数值格式控制符对照表\n\n\n\n\n\n\n\n\n\n控制符\n含义\n示例\n输出示例\n\n\n\n\n.2f\n保留两位小数\n{3.1415:.2f}\n3.14\n\n\n,\n添加千位分隔符\n{1000000:,}\n1,000,000\n\n\n%\n百分比表示，自动 ×100 并加 %\n{0.85:%}\n85.000000%\n\n\n.1%\n百分比保留一位小数\n{0.85:.1%}\n85.0%\n\n\n&gt;10\n右对齐，占 10 个字符宽度\n\"{'abc':&gt;10}\"\n'       abc'\n\n\n&lt;10\n左对齐，占 10 个字符宽度\n\"{'abc':&lt;10}\"\n'abc       '\n\n\n^10\n居中对齐，占 10 个字符宽度\n\"{'abc':^10}\"\n'   abc    '\n\n\n0&gt;5\n用 0 补齐左侧，宽度为 5\n{42:0&gt;5}\n00042",
    "crumbs": [
      "语法格式",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>格式化字符串：f-string 格式</span>"
    ]
  },
  {
    "objectID": "body/print_02_format.html#示例集锦",
    "href": "body/print_02_format.html#示例集锦",
    "title": "格式化字符串：f-string 格式",
    "section": "3. 示例集锦",
    "text": "3. 示例集锦\n\n3.1 金额 + 小数 + 千分位\nprice = 8700.495\nprint(f\"单价为 ￥{price:,.2f} 元\")\n# 输出：单价为 ￥8,700.50 元\n注意：本例中，: 后面设定了两种格式控制符，一个是 , (添加千分位符)；另一个是 .2f (保留两位小数，自动四舍五入)。\n\nprice = 8700.495\nprint(f\"单价为 ￥{price:,.2f} 元\")\n\n单价为 ￥8,700.50 元\n\n\n\n\n3.2 百分比表达\nrate = 0.0826\nprint(f\"年化收益率为 {rate:.2%}\")\n# 输出：\n# 年化收益率为 8.26%\n\n\n3.3 对齐与填充\n\nname = \"Tom\"\nprint(f\"|{name:&lt;10}|\")   # 左对齐\nprint(f\"|{name:^10}|\")   # 居中对齐\nprint(f\"|{name:&gt;10}|\")   # 右对齐\n\n|Tom       |\n|   Tom    |\n|       Tom|\n\n\n\n# 更复杂的例子\n\nnames = [\"Tom\", \"Rebbeca\"]\nages = [25, 30]\nprint(f\"|{'Name':&lt;10}|{'Age':^10}|\")  # 表头\nprint(f\"|{'-'*10}|{'-'*10}|\")         # 分隔线\nprint(f\"|{names[0]:&lt;10}|{ages[0]:^10}|\")  # 第一个人左对齐\nprint(f\"|{names[1]:&lt;10}|{ages[1]:^10}|\")  # 第二个人右对齐\n\n|Name      |   Age    |\n|----------|----------|\n|Tom       |    25    |\n|Rebbeca   |    30    |\n\n\n\n# 进一步扩展：自定义列宽\n\nnames = [\"Tom\", \"Rebbeca\"]\nages = [25, 30]\ncol_width = 15  # 列宽\nprint(f\"|{'Name':&lt;{col_width}}|{'Age':^{col_width}}|\")  # 表头\nprint(f\"|{'-'*col_width}|{'-'*col_width}|\")             # 分隔线\nprint(f\"|{names[0]:&lt;{col_width}}|{ages[0]:^{col_width}}|\")  # 第一列左对齐\nprint(f\"|{names[1]:&lt;{col_width}}|{ages[1]:^{col_width}}|\")  # 第二列居中对齐\n\n|Name           |      Age      |\n|---------------|---------------|\n|Tom            |      25       |\n|Rebbeca        |      30       |\n\n\n需要说明的是，print(f\"|{'-'*{col_width}}|{'-'*{col_width}}|\") 中的\n\nQ = 5\nprint(\"I have Q apples\")\nprint(\"I have {Q} apples\")\nprint(f\"I have {Q} apples\")\n\nI have Q apples\nI have {Q} apples\nI have 5 apples\n\n\n\nprint('~'*10)     \n\ntimes = 10\nprint(f\"'~'*times\")\nprint(f\"'~'*{times}\")\nprint(f\"{'~'*times}\")\n\n# print(f\"{'~'*{times}}\")  # 语法错误\n\n~~~~~~~~~~\n'~'*times\n'~'*10\n~~~~~~~~~~\n\n\n\n\n3.4 数字补零\norder = 42\nprint(f\"订单号：{order:06d}\")  # 总共6位，前面补0\n# 输出：\n# 订单号：000042",
    "crumbs": [
      "语法格式",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>格式化字符串：f-string 格式</span>"
    ]
  },
  {
    "objectID": "body/print_02_format.html#小技巧组合使用",
    "href": "body/print_02_format.html#小技巧组合使用",
    "title": "格式化字符串：f-string 格式",
    "section": "4. 小技巧：组合使用",
    "text": "4. 小技巧：组合使用\n你可以同时使用多个控制符：\nprice = 1234567.89\nprint(f\"￥{price:&gt;15,.2f}\")\n# 输出：\n# ￥     1,234,567.89 （右对齐，千位分隔，保留两位）",
    "crumbs": [
      "语法格式",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>格式化字符串：f-string 格式</span>"
    ]
  },
  {
    "objectID": "body/print_02_format.html#字符串格式控制公式记忆结构",
    "href": "body/print_02_format.html#字符串格式控制公式记忆结构",
    "title": "格式化字符串：f-string 格式",
    "section": "5. 字符串格式控制公式（记忆结构）",
    "text": "5. 字符串格式控制公式（记忆结构）\n[[fill]align][sign][#][0][width][,][.precision][type]\n例如：\nf\"{num:0&gt;10,.2f}\"\n解释如下：\n\n0：填充字符\n&gt;：右对齐\n10：宽度为 10\n,：加千分位\n.2f：保留两位小数",
    "crumbs": [
      "语法格式",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>格式化字符串：f-string 格式</span>"
    ]
  },
  {
    "objectID": "body/print_02_format.html#小结常见组合模板",
    "href": "body/print_02_format.html#小结常见组合模板",
    "title": "格式化字符串：f-string 格式",
    "section": "6. 小结：常见组合模板",
    "text": "6. 小结：常见组合模板\n\n\n\n任务\n格式说明符\n\n\n\n\n金额显示（带逗号）\n:,.2f\n\n\n百分比\n:.1%\n\n\n补零编号\n:05d 或 :06d\n\n\n对齐文字输出\n:&gt;10, :&lt;10, :^10",
    "crumbs": [
      "语法格式",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>格式化字符串：f-string 格式</span>"
    ]
  },
  {
    "objectID": "body/print_02_format.html#推荐文档阅读",
    "href": "body/print_02_format.html#推荐文档阅读",
    "title": "格式化字符串：f-string 格式",
    "section": "7. 推荐文档阅读",
    "text": "7. 推荐文档阅读\n\n官方格式说明：https://docs.python.org/3/library/string.html#format-specification-mini-language\nPython f-string 教程扩展：https://realpython.com/python-f-strings/\n\n如果你希望我整理一个 格式控制速查表 PDF 或 .md 文件版本，我可以帮你制作一份，可随时查阅使用。需要吗？\n\nimport matplotlib.pyplot as plt\n\n# Generate data\nt = [1, 2, 3, 4, 5]\ny = [10, 20, 30, 40, 50]\n\n# Plot scatter plot\nplt.figure(figsize=(4, 3))\nplt.scatter(t, y)\nplt.xlabel('t')\nplt.ylabel('y')\nplt.title('Scatter Plot of t vs y')\nplt.show()",
    "crumbs": [
      "语法格式",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>格式化字符串：f-string 格式</span>"
    ]
  },
  {
    "objectID": "body/C05-常用单调凸模型-边际递减.html",
    "href": "body/C05-常用单调凸模型-边际递减.html",
    "title": "单调递增且凹的函数及其应用",
    "section": "",
    "text": "1. 简介\n在经济学中，单调递增且凹的函数（即满足 \\(Q'(s) &gt; 0\\) 且 \\(Q''(s) &lt; 0\\) 的函数）广泛应用于描述边际递减效应。这些函数具有递增的特性，但随着自变量 \\(s\\) 的增加，函数增速逐渐放缓。无论是在效用理论、生产函数，还是投资回报、技术进步等领域，理解这种函数形式都能帮助我们更好地把握经济系统中的递减效益现象。\n本讲义将介绍几种常见的符合条件 \\(Q'(s) &gt; 0\\) 且 \\(Q''(s) &lt; 0\\) 的经济学模型，并讨论其在实际研究中的应用。每个模型都将结合经典文献、最新研究以及 Python 实现，以帮助读者深刻理解这些函数的经济含义和实际应用。",
    "crumbs": [
      "语法格式",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>单调递增且凹的函数及其应用</span>"
    ]
  },
  {
    "objectID": "body/C05-常用单调凸模型-边际递减.html#简介",
    "href": "body/C05-常用单调凸模型-边际递减.html#简介",
    "title": "单调递增且凹的函数及其应用",
    "section": "",
    "text": "2.1 模型 1：对数效用函数\n模型设定： 对数效用函数 \\(U(x) = \\ln(x)\\) 是经济学中经典的边际效用递减模型。该函数表明，随着消费量 \\(x\\) 的增加，消费者从每单位额外消费中获得的效用递减。即，消费者越消费，其边际效用越小。\n\n参数取值范围：\\(x &gt; 0\\)。\n经济含义：效用函数的凹性反映了消费者对商品的需求递减效应，即消费者对于初期的消费非常感兴趣，但随着消费量的增加，额外的效用增加越来越小。\n\n\n经典文献：\n\nCamerer, C., Loewenstein, G., & Rabin, M. (2004). “Advances in Behavioral Economics.” Princeton University Press.\n该书讨论了行为经济学中的边际效用递减现象，并通过对数效用函数 \\(U(x) = \\ln(x)\\) 描述了消费者在面对不同选择时的决策过程。书中利用该函数展示了消费者如何在有限资源的情况下平衡消费与储蓄，特别是在风险和不确定性下的消费行为。\n\n\n\n扩展阅读：\n\nHarberger, A. C. (1962). “The Measurement of Welfare.” The American Economic Review, 52(3), 42-58.\nHarberger 使用对数效用函数探讨了福利的测量，并提出如何通过效用函数计算消费者剩余。通过该模型，研究者能够量化消费与福利之间的关系，揭示了边际效用递减对福利评估的影响。\n函数形式：\\(U(x) = \\ln(x)\\)\n参数含义：\\(x\\) 代表消费量，\\(U(x)\\) 代表效用，反映了消费者的边际效用递减现象。\nLink, PDF, Google\nKőszegi, B., & Rabin, M. (2006). “A Model of Reference-Dependent Preferences.” The Quarterly Journal of Economics, 121(4), 1133-1165.\n本文提出了参照依赖模型，使用对数效用函数来分析消费者如何根据参照点变化调整消费决策，探讨了参照点和边际效用递减的互动效应。\n函数形式：\\(U(x) = \\ln(x)\\)，结合参照依赖效应的函数模型\n参数含义：\\(x\\) 为消费量，体现了参照点对消费者效用评估的影响。\nLink, PDF, Google\n\n\n\n\n2.2 模型 2：Cobb-Douglas 生产函数\n模型设定： Cobb-Douglas 生产函数广泛应用于描述生产中的规模报酬递减现象。其基本形式为：\n\\[\nY = A K^\\alpha L^\\beta\n\\]\n其中，\\(Y\\) 代表总产出，\\(K\\) 和 \\(L\\) 分别代表资本和劳动投入，\\(A\\) 为技术水平，\\(\\alpha\\) 和 \\(\\beta\\) 为资本和劳动的产出弹性。\n\n参数取值范围：\\(0 &lt; \\alpha, \\beta &lt; 1\\)，且 \\(\\alpha + \\beta = 1\\)。\n经济含义：该模型体现了随着资本和劳动投入的增加，边际产出逐渐递减，即规模报酬递减现象。Cobb-Douglas 生产函数能够有效地描述长期经济增长过程中的生产要素作用。\n\n\n经典文献：\n\nSolow, R. M. (1956). “A Contribution to the Theory of Economic Growth.” The Quarterly Journal of Economics, 70(1), 65-94.\nSolow 提出了经济增长理论，并使用 Cobb-Douglas 生产函数描述了资本和劳动在经济增长中的作用。他的模型揭示了资本积累和技术进步如何影响长期经济增长，以及规模报酬递减如何影响生产率的提高。\nLink, PDF, Google\n\n\n\n扩展阅读：\n\nHicks, J. R. (1932). “The Theory of Wages.” Macmillan.\nHicks 研究了工资和劳动市场的关系，使用 Cobb-Douglas 函数来表示生产中的资本和劳动之间的相互作用，探讨了生产函数中的规模报酬递减效应对工资和就业的影响。\n函数形式：\\(Y = A K^\\alpha L^\\beta\\)\n参数含义：\\(\\alpha\\) 和 \\(\\beta\\) 为资本和劳动的产出弹性，反映了生产要素的贡献。\nLink, PDF, Google\nFoster, L., Haltiwanger, J., & Krizan, C. J. (2006). “Market Selection, Reallocation, and Restructuring in the U.S. Retail Trade Sector in the 1990s.” The Review of Economics and Statistics, 88(4), 748-758.\n该文献利用 Cobb-Douglas 生产函数分析了美国零售行业的市场选择和资源重新配置，探讨了资本和劳动在经济重组中的重要性，以及规模报酬递减对企业效率的影响。\n函数形式：\\(Y = A K^\\alpha L^\\beta\\)\n参数含义：资本和劳动的产出弹性 \\(\\alpha\\) 和 \\(\\beta\\)，反映了市场和生产要素在零售行业重组中的作用。\nLink, PDF, Google\n\n\n\n\n2.3 模型 3：技术进步的累积效应模型\n模型设定： 技术进步的累积效应通常使用幂函数形式来表示。其基本形式为：\n\\[\nA(t) = t^\\alpha\n\\]\n其中，\\(A(t)\\) 代表技术进步，\\(t\\) 代表时间，\\(\\alpha\\) 为技术进步的增速。技术进步的累积效应表明，技术进步随时间的推移而增加，但其增速逐渐减缓。\n\n参数取值范围：\\(\\alpha &gt; 0\\)。\n经济含义：该模型反映了技术进步随着时间的推移不断提高生产力，但增速逐渐减缓，表现为技术进步的边际效益递减。\n\n\n经典文献：\n\nRomer, P. M. (1990). “Endogenous Technological Change.” The Journal of Political Economy, 98(5), S71-S102.\nRomer 在其内生增长模型中使用了技术进步的幂函数形式，分析了技术进步如何推动经济增长。文中指出，技术进步虽逐步减缓增速，但仍持续推动生产力和产出的提升。\nLink, PDF, Google\n\n\n\n扩展阅读：\n\nNelson, R. R., & Winter, S. G. (1982). “An Evolutionary Theory of Economic Change.” Belknap Press.\n本书详细讨论了技术变革和创新在经济发展中的作用，使用了类似的幂函数形式来描述技术进步随时间的累积效应，并分析了如何通过技术进步提升生产力。\n函数形式：\\(A(t) = t^\\alpha\\)\n参数含义：\\(A(t)\\) 代表技术进步，\\(\\alpha\\) 为技术进步的增速。\nLink, PDF, Google\nAghion, P., & Howitt, P. (2009). “The Economics of Growth.” MIT Press.\n这本书讨论了内生经济增长的理论，使用了技术进步的幂函数形式，分析了技术进步在推动经济增长中的作用。文中深入探讨了如何通过技术进步的递增效应实现长期经济增长。\n函数形式：\\(A(t) = t^\\alpha\\)\n参数含义：\\(A(t)\\) 代表技术进步，\\(\\alpha\\) 为技术进步的增速。\nLink, PDF, Google\n\n\n\n\n2.4 模型 4：投资回报率递减模型\n模型设定： 投资回报率递减通常使用幂函数形式来表示，反映了随着投资量的增加，回报的增速逐渐放缓。其基本形式为：\n\\[\nR(I) = I^\\alpha\n\\]\n其中，\\(R(I)\\) 代表投资回报率，\\(I\\) 代表投资额，\\(\\alpha\\) 为回报递减系数。\n\n参数取值范围：\\(0 &lt; \\alpha &lt; 1\\)。\n经济含义：当 \\(\\alpha\\) 小于 1 时，投资回报率随着投资量的增加而递减，表明初期投资带来的回报较大，但随着投资的增加，回报逐渐减小。\n\n\n经典文献：\n\nLucas, R. E. (1988). “On the Mechanics of Economic Development.” Journal of Monetary Economics, 22(1), 3-42.\n本文使用了类似的幂函数形式来描述资本积累的回报递减效应，并将其应用于经济发展模型中。文章分析了资本积累与投资回报之间的关系，并展示了随着投资增加，回报如何逐渐减小。\nLink, PDF, Google\n\n\n\n扩展阅读：\n\nAcemoglu, D. (2009). “Introduction to Modern Economic Growth.” Princeton University Press.\n本书深入探讨了经济增长理论，使用了幂函数模型来描述投资回报率递减效应，分析了资本积累、技术进步与生产效率的相互作用。\n函数形式：\\(R(I) = I^\\alpha\\)\n参数含义：\\(I\\) 为投资额，\\(\\alpha\\) 为回报递减系数。\nLink, PDF, Google\nGertler, P., & Gilchrist, S. (2018). “What Happened to US Unemployment during the Great Recession?” Brookings Papers on Economic Activity.\n本文使用幂函数来描述资本的回报递减效应，研究了投资对经济周期的影响，并讨论了回报递减如何影响企业的投资决策。\n函数形式：\\(R(I) = I^\\alpha\\)\n参数含义：\\(I\\) 为投资，\\(\\alpha\\) 为回报递减系数。\nLink, PDF, Google\n\n\n\n\n2.5 模型 5：跨期消费决策模型\n模型设定： 跨期消费决策模型通常使用折现效用函数来描述消费者如何在当前消费与未来消费之间做出权衡。其基本形式为：\n\\[\nU(C_t) = \\sum_{t=0}^\\infty \\beta^t C_t\n\\]\n其中，\\(C_t\\) 代表第 \\(t\\) 期的消费，\\(\\beta\\) 为折现因子，反映了消费者对未来效用的偏好。\n\n参数取值范围：\\(0 &lt; \\beta &lt; 1\\)。\n经济含义：折现因子 \\(\\beta\\) 反映了消费者对未来消费的偏好，值越接近 1 表示消费者更加重视未来效用，值较小则表示消费者偏好当前消费。\n\n\n经典文献：\n\nStrotz, R. H. (1955). “Myopia and Inconsistency in Dynamic Utility Maximization.” The Review of Economic Studies, 23(3), 165-180.\n本文提出了跨期消费中的时间不一致性问题，讨论了消费者如何在当前和未来之间做出选择，并使用折现效用函数来描述跨期效用。\nLink, PDF, Google\n\n\n\n扩展阅读：\n\nRamsey, F. P. (1928). “A Mathematical Theory of Saving.” The Economic Journal, 38(152), 543-559. Link, PDF, Google\nRamsey 提出了跨期最优储蓄模型，并分析了折现效用函数在消费者跨期决策中的应用，探讨了储蓄与消费之间的关系。\n函数形式：\\(U(C_t) = \\sum_{t=0}^\\infty \\beta^t C_t\\)\n参数含义：\\(C_t\\) 为第 \\(t\\) 期消费，\\(\\beta\\) 为折现因子，反映时间偏好。\nLaibson, D. (2015). “The Economics of Intertemporal Choice and the Science of Self-Control.” The Journal of Economic Perspectives, 29(3), 151-172. Link, PDF, Google\n本文探讨了跨期选择的经济学，分析了消费者在面对当前和未来消费的抉择时如何折现效用，并解释了跨期不一致性对经济决策的影响。\n函数形式：\\(U(C_t) = \\sum_{t=0}^\\infty \\beta^t C_t\\)\n参数含义：\\(C_t\\) 为消费，\\(\\beta\\) 为折现因子。\n\n\n# Model 5: 折现效用函数\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 定义折现效用函数\ndef discount_utility(C, beta):\n    return np.sum(beta**np.arange(len(C)) * C)\n\n# 生成消费序列\nC = np.array([10, 12, 15, 18, 22])\nbeta_values = [0.8, 0.9, 0.95]\n\n# 创建图形\nfig, axes = plt.subplots(1, 2, figsize=(8, 3.5))  # 设置左右图布局和尺寸\n\n# 左图：折现效用函数随时间变化\nfor beta in beta_values:\n    discounted_utilities = [\n        discount_utility(C[:t+1], beta) for t in range(len(C))\n    ]\n    axes[0].plot(\n        range(1, len(C)+1), discounted_utilities,\n        label=rf'$\\beta = {beta}$'\n    )\n\naxes[0].set_title(\n    r'Discounted Utility Over Time: $U(C_t) = \\sum_{t=0}^\\infty \\beta^t C_t$',\n    fontsize=10\n)\naxes[0].set_xlabel('Time Period (t)', fontsize=9)\naxes[0].set_ylabel('Discounted Utility', fontsize=9)\naxes[0].set_xticks(range(1, 6))\naxes[0].set_xticklabels(['1', '2', '3', '4', '5'])\naxes[0].legend(fontsize=9)\naxes[0].grid(color='lightgray')\naxes[0].text(\n    2.1, 65, r'$C_t$ values: [10, 12, 15, 18, 22]',\n    fontsize=9, horizontalalignment='left', verticalalignment='center'\n)\n\n# 右图：$\\beta^t$ 与 $t$ 的关系\nt = np.arange(0, 10)  # 时间范围\nfor beta in beta_values:\n    beta_t = beta**t\n    axes[1].plot(t, beta_t, label=rf'$\\beta = {beta}$')\n\naxes[1].set_title(\n    r'Relationship Between $\\beta^t$ and $t$', fontsize=10\n)\naxes[1].set_xlabel('Time Period (t)', fontsize=9)\naxes[1].set_ylabel(r'$\\beta^t$', fontsize=9)\naxes[1].legend(fontsize=9)\naxes[1].grid(color='lightgray')\n\n# 自动调整布局\nplt.tight_layout()\n\n# 保存图片\noutput_dir = r'D:\\JG\\助教推文提交\\2020助教推文\\_00lian_blogs\\Figs'\nFigName = 'model_marginal_decrease_05.png'\nos.makedirs(output_dir, exist_ok=True)\nplt.savefig(os.path.join(output_dir, FigName), dpi=150, bbox_inches='tight')\n\n# 显示图形\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n2.6 模型 6：环保与资源消耗模型\n模型设定： 环保和资源消耗模型通常使用类似于 \\(Q(s) = 1 - e^{-\\beta s}\\) 的函数来描述资源消耗对社会效用或环境的影响。该函数表明，随着资源消耗 \\(s\\) 的增加，环境效用或社会福利的增加会逐渐减缓。\n\n参数取值范围：\\(\\beta &gt; 0\\)。\n经济含义：随着资源消耗的增加，社会效用的增幅逐渐放缓，体现了边际效益递减的现象。\n\n\n经典文献：\n\nBarbier, E. B. (1989). “The Economic Dynamics of the Environment.” Cambridge University Press.\n该书探讨了环境经济学中的资源消耗模型，应用了类似于 \\(Q(s) = 1 - e^{-\\beta s}\\) 的函数形式，分析了资源消耗对经济社会效益的影响。\nLink, PDF, Google\n\n\n\n扩展阅读：\n\nDasgupta, P., & Mäler, K. G. (2000). “The Economics of Non-renewable Resources.” Environmental and Resource Economics, 16(4), 431-458. Link, PDF, Google\n本文讨论了不可再生资源的经济学，使用了类似于 \\(Q(s) = 1 - e^{-\\beta s}\\) 的函数来描述资源消耗对经济社会福利的递减效应。\n函数形式：\\(Q(s) = 1 - e^{-\\beta s}\\)\n参数含义：\\(s\\) 为资源消耗，\\(\\beta\\) 为影响效用的参数。\nGersbach, H. (2012). “Environmental Policy and Market Dynamics.” Environmental and Resource Economics, 52(2), 251-276. Link, PDF, Google 本文探讨了环保政策如何影响资源消耗，采用了资源消耗递减的模型，并分析了环境政策对市场动态的影响。\n函数形式：\\(Q(s) = 1 - e^{-\\beta s}\\)\n参数含义：\\(s\\) 为资源消耗，\\(\\beta\\) 为影响社会福利的参数。\n\n\n\n\n分析及文献支持\n在环境与资源经济学中，函数形式 \\(Q(s) = 1 - e^{-\\beta s}\\) 常用于描述资源消耗 \\(s\\) 对社会效用或环境质量的影响过程。该函数具有单调递增、边际效应递减的性质，即资源投入越多，带来的效用增加幅度越小。参数 \\(\\beta &gt; 0\\) 控制效用的“增长速度”或“饱和速度”：\n\n当 \\(\\beta\\) 较大时，效用函数在较小的 \\(s\\) 上就迅速趋近于饱和，意味着较少的资源消耗即可实现效用的显著增长，常用于模拟资源利用效率较高或环境承载力较弱的情形；\n当 \\(\\beta\\) 较小时，效用需要在更大范围的资源消耗下才能显著提升，反映出较缓慢的响应过程，例如生态系统恢复较慢或边际资源利用成本较低。\n\n需要说明的是，下列文献中的模型虽然不一定直接采用 \\(Q(s) = 1 - e^{-\\beta s}\\) 的形式，但其结构和设定与该类函数具有高度一致性，故可作为相关背景参考。\n文献中用于实际建模的 \\(\\beta\\) 值差异较大，一般取决于资源种类、环境系统的动态响应特性以及模型所处的时间尺度。综合已有研究，常见的取值范围大致为 \\(0.1 \\leq \\beta \\leq 2.0\\)，具体如下：\n\n1. 污染累积与环境吸收模型\n该类模型常用于描述污染物在环境中的积累与衰减过程，\\(\\beta\\) 代表环境对污染的吸收速度或自净能力。\n\nNordhaus (1991) 在其温室气体经济模型中采用指数结构来刻画碳排放对气候的长期影响，参数校准结果约为 \\(\\beta \\approx 0.2\\)，反映碳循环中大气与海洋交换的缓慢过程。\nNordhaus, W. D. (1991). To Slow or Not to Slow: The Economics of the Greenhouse Effect. The Economic Journal, 101(407), 920–937. Link, PDF, Google\nXepapadeas (1997) 在不同地区的工业污染政策模型中设定 \\(\\beta \\in [0.5, 1.5]\\)，用于模拟环境自净能力的区域异质性。\nXepapadeas, A. (1997). Advanced Principles in Environmental Policy. Edward Elgar Publishing. Link, PDF, Google\n\n\n\n2. 可再生资源管理模型\n在渔业、森林等模型中，\\(\\beta\\) 通常反映种群或资源的再生速度。\n\nClark (1990) 使用指数型增益函数刻画鱼类资源的密度依赖性增长过程，实证模型中估计 \\(\\beta \\approx 0.8\\)。\nClark, C. W. (1990). Mathematical Bioeconomics: Optimal Management of Renewable Resources (2nd ed.). Wiley. Link, PDF, Google\nConrad (2010) 在分析森林采伐与恢复动态时设定 \\(\\beta = 1.2\\)，用于刻画热带雨林在高强度砍伐压力下的敏感响应。\nConrad, J. M. (2010). Resource Economics. Cambridge University Press. Link, PDF, Google\n\n\n\n3. 能源与矿产资源消耗模型\n在不可再生资源领域，\\(\\beta\\) 常用于衡量资源消耗对长期社会福利的边际贡献或替代技术的渗透速度。\n\nDasgupta & Heal (1979) 提出的资源枯竭模型中，假设 \\(\\beta \\in [0.3, 0.6]\\)，用于模拟化石能源消耗对社会效用的长期边际递减趋势。\nDasgupta, P., & Heal, G. (1979). Economic Theory and Exhaustible Resources. Cambridge University Press. Link, PDF, Google\nStern (2006) 在其气候变化报告中，采用 \\(\\beta \\approx 0.1\\) 来表示替代能源技术的低响应性，强调绿色转型的延迟效应。\nStern, N. (2006). Stern Review on the Economics of Climate Change. HM Treasury. Link, PDF, Google\n\n\n\n\n总结\n参数 \\(\\beta\\) 的常见取值范围为 \\(0.1 \\leq \\beta \\leq 2.0\\)，具体数值取决于所研究对象的响应速度、制度情景设定以及数据可得性。较小的 \\(\\beta\\)（如 \\(0.1–0.5\\)）适用于响应较慢的系统，如气候、碳汇等；较大的 \\(\\beta\\)（如 1–2）则更常用于高频、短期响应系统，如渔业或污染治理。\n在实际应用中，研究者常通过以下方法估计或设定 \\(\\beta\\) 的取值： 1. 基于历史数据拟合（如资源消耗与生态指标的关系）； 2. 借助实验观测（如生态恢复速度的野外数据）； 3. 结合理论设定与情景分析（如政策模拟中的敏感性检验）。\n\n\n4. 模型对比总结\n以下表格总结了各个模型设定的形式、参数取值范围、参数含义及适用场景：\n\n\n\n\n\n\n\n\n\n\n模型类型\n函数形式\n参数取值范围\n参数含义\n适用场景\n\n\n\n\n边际效用递减\n对数函数：\\(U(x) = \\ln(x)\\)\n\\(x &gt; 0\\)\n\\(x\\) 为消费量，表示消费的边际效用递减\n消费者选择行为、商品需求分析、福利经济学\n\n\n生产函数的规模报酬递减\nCobb-Douglas：\\(Y = A K^\\alpha L^\\beta\\)\n\\(0 &lt; \\alpha, \\beta &lt; 1\\)\n\\(\\alpha\\), \\(\\beta\\) 为资本和劳动的产出弹性\n生产经济学、企业生产过程、宏观经济增长模型\n\n\n技术进步的累积效应\n幂函数：\\(A(t) = t^\\alpha\\)\n\\(\\alpha &gt; 0\\)\n\\(A(t)\\) 为技术进步，\\(\\alpha\\) 为技术的增速\n技术进步模型、经济增长理论、创新经济学\n\n\n投资回报率递减\n幂函数：\\(R(I) = I^\\alpha\\)\n\\(0 &lt; \\alpha &lt; 1\\)\n\\(I\\) 为投资，\\(\\alpha\\) 为回报递减系数\n投资分析、资本积累模型、企业决策与增长\n\n\n跨期消费决策\n折现效用：\\(U(C_t) = \\sum_{t=0}^\\infty \\beta^t C_t\\)\n\\(0 &lt; \\beta &lt; 1\\)\n\\(C_t\\) 为第 \\(t\\) 期消费，\\(\\beta\\) 为时间偏好折现因子\n跨期消费与储蓄、经济学中的消费者行为模型\n\n\n环保与资源消耗模型\n资源消耗：\\(Q(s) = 1 - e^{-\\beta s}\\)\n\\(\\beta &gt; 0\\)\n\\(s\\) 为资源消耗，\\(\\beta\\) 为资源消耗影响因子\n环境经济学、可持续发展、资源消耗与污染模型\n\n\n\n\n\n5. 总结\n在文介绍了满足 \\(Q'(s) &gt; 0\\) 且 \\(Q''(s) &lt; 0\\) 的 6 种常见经济学模型，并提供了经典文献和最新研究的引用，帮助读者理解这些模型的经济含义和应用场景。通过 Python 实现，我们展示了每个模型在不同参数下的可视化效果，增强了对模型行为的直观理解。\n感谢您的进一步指示！现在我将参考文献按首字母顺序排列，并使用您要求的引文格式。以下是调整后的版本：\n\n\n6. 参考文献\n\nAcemoglu, D. (2009). Introduction to Modern Economic Growth. MIT Press. Link, PDF, Google\nAghion, P., & Howitt, P. (2009). The Economics of Growth. MIT Press. Link, PDF, Google\nBarbier, E. B. (1989). The Economic Dynamics of the Environment. Cambridge University Press. Link, PDF, Google\nCooper, R. W., & Haltiwanger, J. C. (2006). On the Nature of Capital Adjustment Costs. Review of Economic Studies, 73(3), 611–633. Link, PDF, Google\nDasgupta, P., & Mäler, K. G. (2000). The Economics of Non-renewable Resources. Environmental and Resource Economics, 16(4), 431-458. Link, PDF, Google\nFoster, L., Haltiwanger, J., & Krizan, C. J. (2006). Market Selection, Reallocation, and Restructuring in the U.S. Retail Trade Sector in the 1990s. The Review of Economics and Statistics, 88(4), 748-758. Link, PDF, Google\nGersbach, H. (2012). Environmental Policy and Market Dynamics. Environmental and Resource Economics, 52(2), 251-276. Link, PDF, Google\nGertler, P., & Gilchrist, S. (2018). What Happened to US Unemployment during the Great Recession? Brookings Papers on Economic Activity. Link, PDF, Google\nHicks, J. R. (1932). The Theory of Wages. Macmillan. Link, PDF, Google\nKőszegi, B., & Rabin, M. (2006). A Model of Reference-Dependent Preferences. The Quarterly Journal of Economics, 121(4), 1133-1165. Link, PDF, Google\nLaibson, D. (2015). The Economics of Intertemporal Choice and the Science of Self-Control. The Journal of Economic Perspectives, 29(3), 151-172. Link, PDF, Google\nLucas, R. E. (1988). On the Mechanics of Economic Development. Journal of Monetary Economics, 22(1), 3-42. Link, PDF, Google\nNelson, R. R., & Winter, S. G. (1982). An Evolutionary Theory of Economic Change. Belknap Press. Link, PDF, Google\nRamsey, F. P. (1928). A Mathematical Theory of Saving. The Economic Journal, 38(152), 543-559. Link, PDF, Google\nRomer, P. M. (1990). Endogenous Technological Change. The Journal of Political Economy, 98(5), S71-S102. Link, PDF, Google\nStrotz, R. H. (1955). Myopia and Inconsistency in Dynamic Utility Maximization. The Review of Economic Studies, 23(3), 165-180. Link, PDF, Google\nSolow, R. M. (1956). A Contribution to the Theory of Economic Growth. The Quarterly Journal of Economics, 70(1), 65-94. Link, PDF, Google\nStrotz, R. H. (1955). “Myopia and Inconsistency in Dynamic Utility Maximization.” The Review of Economic Studies, 23(3), 165-180. Link, PDF, Google\nBarbier, E. B. (1989). “The Economic Dynamics of the Environment.” Cambridge University Press. Link, PDF, Google\nGertler, P., & Gilchrist, S. (2018). “What Happened to US Unemployment during the Great Recession?” Brookings Papers on Economic Activity. Link, PDF, Google",
    "crumbs": [
      "语法格式",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>单调递增且凹的函数及其应用</span>"
    ]
  },
  {
    "objectID": "body/C05-常用单调凸模型-边际递减.html#附录python-代码",
    "href": "body/C05-常用单调凸模型-边际递减.html#附录python-代码",
    "title": "单调递增且凹的函数及其应用",
    "section": "附录：Python 代码",
    "text": "附录：Python 代码\n\nA2.1 对数效用函数 \\(U(x) = \\ln(x)\\)\n通过上面的代码，我们可以绘制出效用函数 \\(U(x) = \\ln(x)\\) 的图形，帮助我们直观理解边际效用递减的现象。\n\n# model 1：对数效用函数 U(x) = a * ln(x)\n\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\n\n# 定义效用函数\ndef utility(x, a):\n    return a * np.log(x)\n\n# 绘制不同 a 值的图形\nplt.figure(figsize=(4, 3))  # 设置图片尺寸\n\n# Define the range of x and a_values\nx = np.linspace(1, 10, 100)  # x values from 1 to 10\na_values = [1, 2, 3]  # Different values of a\n\n# Loop through a_values and plot the utility function\nfor a in a_values:\n    y = utility(x, a)\n    plt.plot(x, y, label=f'$a = {a}$')\n\n# 使用 LaTeX 格式设置标题和轴标签\nplt.title(r'Utility Function: $U(x) = a \\cdot \\ln(x)$', fontsize=10)\nplt.xlabel('Consumption ($x$)', fontsize=10)\nplt.ylabel(r'Utility ($U(x)$)', fontsize=10)\nplt.legend()\nplt.grid(color='lightgray')\n\n# 自动调整布局以防止标题或标签被截断\nplt.tight_layout()\n\n# 保存图片\noutput_dir = r'D:\\JG\\助教推文提交\\2020助教推文\\_00lian_blogs\\Figs'\nFigName = 'model_marginal_decrease_01.png'\nos.makedirs(output_dir, exist_ok=True)\nplt.savefig(os.path.join(output_dir, FigName), dpi=150, bbox_inches='tight')\n\n# 显示图形\nplt.show()\n\n\n\n\n\n\n\n\n\n# Model 2: Cobb-Douglas 生产函数和单因素生产函数\n\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\n\n# 定义 Cobb-Douglas 生产函数的参数\nA = 1  # 技术水平\nalpha = 0.3  # 资本弹性\nbeta  = 0.7  # 劳动弹性\n\n# 生成资本 (K) 和劳动 (L) 的网格\nK = np.linspace(1, 10, 20)\nL = np.linspace(1, 10, 20)\nK, L = np.meshgrid(K, L)\n\n# 计算 Cobb-Douglas 生产函数的输出 (Y)\nY = A * (K**alpha) * (L**beta)\n\n# 定义单因素生产函数的参数\nK_single = np.linspace(1, 10, 100)\nY_single = A * (K_single**alpha)\n\nfig = plt.figure(figsize=(10, 4))  # 增加整体图片尺寸以加大横向间距\n\n# 左图：Cobb-Douglas 生产函数 3D 图形\nax1 = fig.add_subplot(121, projection='3d')\nax1.plot_surface(K, L, Y, cmap='viridis')\nax1.set_title(fr'C-D Function: $Y = AK^\\alpha L^\\beta$ ($\\alpha=${alpha}, $\\beta=${beta})', fontsize=10)\nax1.set_xlabel('Capital (K)', fontsize=10)\nax1.set_ylabel('Labor (L)', fontsize=10)\nax1.set_zlabel('Output (Y)', fontsize=10)\n\n# 右图：单因素生产函数 Y = A K^alpha，缩小为 0.7 倍\nax2 = fig.add_axes([0.6, 0.2, 0.28, 0.6])  # 自定义位置和大小，缩小为 0.7 倍\nax2.plot(K_single, Y_single, label=fr'$Y = AK^\\alpha$ ($\\alpha$={alpha})', color='blue')  \nax2.set_title('Single Input Factor: $Y = AK^\\\\alpha$', fontsize=10)\nax2.set_xlabel('Capital ($K$)', fontsize=10)\nax2.set_ylabel('Output ($Y$)', fontsize=10)\nax2.grid(color = 'lightgray')\nax2.legend()\n\n# 自动调整布局\nplt.tight_layout()\n\n# 保存图片\noutput_dir = r'D:\\JG\\助教推文提交\\2020助教推文\\_00lian_blogs\\Figs'\nFigName = 'model_marginal_decrease_02.png'\nos.makedirs(output_dir, exist_ok=True)\nplt.savefig(os.path.join(output_dir, FigName), dpi=150, bbox_inches='tight')\n\n# 显示图形\nplt.show()\n\nC:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_20688\\2301014185.py:44: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n  plt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n# Model 3: 定义技术进步函数\n\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\n\n# 定义技术进步函数\ndef tech_progress(t, alpha):\n    return t**alpha\n\n# 生成 t 范围\nt = np.linspace(1, 5, 100)\n\n# 设置不同的 alpha 值\nalpha_values = [0.3, 0.5, 0.8]\n\n# 绘制图形\nplt.figure(figsize=(4, 3))  # 设置图片尺寸\n\nfor alpha in alpha_values:\n    A = tech_progress(t, alpha)\n    plt.plot(t, A, label=rf'$\\alpha = {alpha}$')\n\n# 设置标题和轴标签，使用 LaTeX 格式\nplt.title(r'Technology Progress: $A(t) = t^\\alpha$', fontsize=10)\nplt.xlabel('Time ($t$)', fontsize=10)\nplt.ylabel('Technology Progress ($A(t)$)', fontsize=10)\nplt.legend(fontsize=9)\nplt.grid(color='lightgray')\n\n# 保存图片\noutput_dir = r'D:\\JG\\助教推文提交\\2020助教推文\\_00lian_blogs\\Figs'\nFigName = 'model_marginal_decrease_03.png'\nos.makedirs(output_dir, exist_ok=True)\nplt.savefig(os.path.join(output_dir, FigName), dpi=150, bbox_inches='tight')\n\n# 显示图形\nplt.show()\n\n\n\n\n\n\n\n\n\n# Model 4: Investment Return Model\n# This model demonstrates the relationship between investment and return\n# using a power function R(I) = I^alpha, where alpha is the return decay factor.\n\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\n\n# 定义投资回报函数\ndef investment_return(I, alpha):\n    return I**alpha\n\n# 生成投资范围\nI = np.linspace(1, 10, 100)\n\n# 设置不同的 alpha 值\nalpha_values = [0.3, 0.5, 0.7]\n\n# 绘制图形\nplt.figure(figsize=(4, 3))  # 设置图片尺寸\n\nfor alpha in alpha_values:\n    R = investment_return(I, alpha)\n    plt.plot(I, R, label=rf'$\\alpha = {alpha}$')\n\n# 设置标题和轴标签，使用 LaTeX 格式\nplt.title(r'Investment Return: $R(I) = I^\\alpha$', fontsize=10)\nplt.xlabel('Investment ($I$)', fontsize=10)\nplt.ylabel('Return ($R(I)$)', fontsize=10)\nplt.legend(fontsize=9)\nplt.grid(color='lightgray')\n\n# 保存图片\noutput_dir = r'D:\\JG\\助教推文提交\\2020助教推文\\_00lian_blogs\\Figs'\nFigName = 'model_marginal_decrease_04.png'\nos.makedirs(output_dir, exist_ok=True)\nplt.savefig(os.path.join(output_dir, FigName), dpi=180, bbox_inches='tight')\n\n# 显示图形\nplt.show()\n\n\n\n\n\n\n\n\n\n# Model 5: 折现效用函数\n\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\n\n# 定义折现效用函数\ndef discount_utility(C, beta):\n    return np.sum(beta**np.arange(len(C)) * C)\n\n# 生成消费序列\nC = np.array([10, 12, 15, 18, 22])\nbeta_values = [0.8, 0.9, 0.95]\n\n# 创建图形\nfig, axes = plt.subplots(1, 2, figsize=(8, 3.5))  # 设置左右图布局和尺寸\n\n# 左图：折现效用函数随时间变化\nfor beta in beta_values:\n    discounted_utilities = [\n        discount_utility(C[:t+1], beta) for t in range(len(C))\n    ]\n    axes[0].plot(\n        range(1, len(C)+1), discounted_utilities,\n        label=rf'$\\beta = {beta}$'\n    )\n\naxes[0].set_title(\n    r'Discounted Utility Over Time: $U(C_t) = \\sum_{t=0}^\\infty \\beta^t C_t$',\n    fontsize=10\n)\naxes[0].set_xlabel('Time Period (t)', fontsize=9)\naxes[0].set_ylabel('Discounted Utility', fontsize=9)\naxes[0].set_xticks(range(1, 6))\naxes[0].set_xticklabels(['1', '2', '3', '4', '5'])\naxes[0].legend(fontsize=9)\naxes[0].grid(color='lightgray')\naxes[0].text(\n    1.1, 65, r'$C_t$: [10, 12, 15, 18, 22]',\n    fontsize=9, horizontalalignment='left', verticalalignment='center'\n)\n\n# 右图：$\\beta^t$ 与 $t$ 的关系\nt = np.arange(0, 10)  # 时间范围\nfor beta in beta_values:\n    beta_t = beta**t\n    axes[1].plot(t, beta_t, label=rf'$\\beta = {beta}$')\n\naxes[1].set_title(\n    r'Relationship Between $\\beta^t$ and $t$', fontsize=10\n)\naxes[1].set_xlabel('Time Period (t)', fontsize=9)\naxes[1].set_ylabel(r'$\\beta^t$', fontsize=9)\naxes[1].legend(fontsize=9)\naxes[1].grid(color='lightgray')\n\n# 自动调整布局\nplt.tight_layout()\n\n# 保存图片\noutput_dir = r'D:\\JG\\助教推文提交\\2020助教推文\\_00lian_blogs\\Figs'\nFigName = 'model_marginal_decrease_05.png'\nos.makedirs(output_dir, exist_ok=True)\nplt.savefig(os.path.join(output_dir, FigName), dpi=150, bbox_inches='tight')\n\n# 显示图形\nplt.show()\n\n\n\n\n\n\n\n\n\n# Model 6: 资源消耗函数\n\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\n\n# 定义资源消耗函数\ndef resource_consumption(s, beta):\n    return 1 - np.exp(-beta * s)\n\n# 生成资源消耗范围\ns = np.linspace(0, 10, 100)\n\n# 定义三组不同的 beta 值\nbeta_values = [0.3, 0.5, 0.7]\n\n# 绘制图形\nplt.figure(figsize=(4, 3))  # 设置图片尺寸\n\nfor beta in beta_values:\n    Q = resource_consumption(s, beta)\n    plt.plot(s, Q, label=rf'$\\beta = {beta}$')\n\n# 设置标题和轴标签\nplt.title(r'Resource Consumption: $Q(s) = 1 - e^{-\\beta s}$', fontsize=12)\nplt.xlabel('Resource Consumption ($s$)', fontsize=9)\nplt.ylabel('Effect ($Q(s)$)', fontsize=9)\nplt.legend(fontsize=8)\nplt.grid(color='lightgray')\n\n# 保存图片\noutput_dir = r'D:\\JG\\助教推文提交\\2020助教推文\\_00lian_blogs\\Figs'\nFigName = 'model_marginal_decrease_06.png'\nos.makedirs(output_dir, exist_ok=True)\nplt.savefig(os.path.join(output_dir, FigName), dpi=150, bbox_inches='tight')\n\n# 显示图形\nplt.show()\n\n\n\n\n\n\n\n\n提示词：修改为 ln(a x)，绘制a在不同取值下的图形，a = {1，2, 3}； 另外，增加图片尺寸设定的语句； 图片保存到 D:\\2020助教推文_00lian_blogs文件夹下，图片名称为 model_marginal_decrease_01.png，清晰度：180px",
    "crumbs": [
      "语法格式",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>单调递增且凹的函数及其应用</span>"
    ]
  },
  {
    "objectID": "body/05.07-Support-Vector-Machines.html",
    "href": "body/05.07-Support-Vector-Machines.html",
    "title": "In-Depth: Support Vector Machines",
    "section": "",
    "text": "Motivating Support Vector Machines\nThis notebook contains an excerpt from the Python Data Science Handbook by Jake VanderPlas; the content is available on GitHub.\nThe text is released under the CC-BY-NC-ND license, and code is released under the MIT license. If you find this content useful, please consider supporting the work by buying the book!\n&lt; In Depth: Linear Regression | Contents | In-Depth: Decision Trees and Random Forests &gt;\nSupport vector machines (SVMs) are a particularly powerful and flexible class of supervised algorithms for both classification and regression. In this section, we will develop the intuition behind support vector machines and their use in classification problems.\nWe begin with the standard imports:\nAs part of our disussion of Bayesian classification (see In Depth: Naive Bayes Classification), we learned a simple model describing the distribution of each underlying class, and used these generative models to probabilistically determine labels for new points. That was an example of generative classification; here we will consider instead discriminative classification: rather than modeling each class, we simply find a line or curve (in two dimensions) or manifold (in multiple dimensions) that divides the classes from each other.\nAs an example of this, consider the simple case of a classification task, in which the two classes of points are well separated:\nfrom sklearn.datasets.samples_generator import make_blobs\nX, y = make_blobs(n_samples=50, centers=2,\n                  random_state=0, cluster_std=0.60)\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');\nA linear discriminative classifier would attempt to draw a straight line separating the two sets of data, and thereby create a model for classification. For two dimensional data like that shown here, this is a task we could do by hand. But immediately we see a problem: there is more than one possible dividing line that can perfectly discriminate between the two classes!\nWe can draw them as follows:\nxfit = np.linspace(-1, 3.5)\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\nplt.plot([0.6], [2.1], 'x', color='red', markeredgewidth=2, markersize=10)\n\nfor m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:\n    plt.plot(xfit, m * xfit + b, '-k')\n\nplt.xlim(-1, 3.5);\nThese are three very different separators which, nevertheless, perfectly discriminate between these samples. Depending on which you choose, a new data point (e.g., the one marked by the “X” in this plot) will be assigned a different label! Evidently our simple intuition of “drawing a line between classes” is not enough, and we need to think a bit deeper.",
    "crumbs": [
      "语法格式",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>In-Depth: Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "body/05.07-Support-Vector-Machines.html#support-vector-machines-maximizing-the-margin",
    "href": "body/05.07-Support-Vector-Machines.html#support-vector-machines-maximizing-the-margin",
    "title": "In-Depth: Support Vector Machines",
    "section": "Support Vector Machines: Maximizing the Margin",
    "text": "Support Vector Machines: Maximizing the Margin\nSupport vector machines offer one way to improve on this. The intuition is this: rather than simply drawing a zero-width line between the classes, we can draw around each line a margin of some width, up to the nearest point. Here is an example of how this might look:\n\nxfit = np.linspace(-1, 3.5)\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n\nfor m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:\n    yfit = m * xfit + b\n    plt.plot(xfit, yfit, '-k')\n    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none',\n                     color='#AAAAAA', alpha=0.4)\n\nplt.xlim(-1, 3.5);\n\n\n\n\n\n\n\n\nIn support vector machines, the line that maximizes this margin is the one we will choose as the optimal model. Support vector machines are an example of such a maximum margin estimator.\n\nFitting a support vector machine\nLet’s see the result of an actual fit to this data: we will use Scikit-Learn’s support vector classifier to train an SVM model on this data. For the time being, we will use a linear kernel and set the C parameter to a very large number (we’ll discuss the meaning of these in more depth momentarily).\n\nfrom sklearn.svm import SVC # \"Support vector classifier\"\nmodel = SVC(kernel='linear', C=1E10)\nmodel.fit(X, y)\n\nSVC(C=10000000000.0, cache_size=200, class_weight=None, coef0=0.0,\n  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n  max_iter=-1, probability=False, random_state=None, shrinking=True,\n  tol=0.001, verbose=False)\n\n\nTo better visualize what’s happening here, let’s create a quick convenience function that will plot SVM decision boundaries for us:\n\ndef plot_svc_decision_function(model, ax=None, plot_support=True):\n    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n    if ax is None:\n        ax = plt.gca()\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n    \n    # create grid to evaluate model\n    x = np.linspace(xlim[0], xlim[1], 30)\n    y = np.linspace(ylim[0], ylim[1], 30)\n    Y, X = np.meshgrid(y, x)\n    xy = np.vstack([X.ravel(), Y.ravel()]).T\n    P = model.decision_function(xy).reshape(X.shape)\n    \n    # plot decision boundary and margins\n    ax.contour(X, Y, P, colors='k',\n               levels=[-1, 0, 1], alpha=0.5,\n               linestyles=['--', '-', '--'])\n    \n    # plot support vectors\n    if plot_support:\n        ax.scatter(model.support_vectors_[:, 0],\n                   model.support_vectors_[:, 1],\n                   s=300, linewidth=1, facecolors='none');\n    ax.set_xlim(xlim)\n    ax.set_ylim(ylim)\n\n\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\nplot_svc_decision_function(model);\n\n\n\n\n\n\n\n\nThis is the dividing line that maximizes the margin between the two sets of points. Notice that a few of the training points just touch the margin: they are indicated by the black circles in this figure. These points are the pivotal elements of this fit, and are known as the support vectors, and give the algorithm its name. In Scikit-Learn, the identity of these points are stored in the support_vectors_ attribute of the classifier:\n\nmodel.support_vectors_\n\narray([[ 0.44359863,  3.11530945],\n       [ 2.33812285,  3.43116792],\n       [ 2.06156753,  1.96918596]])\n\n\nA key to this classifier’s success is that for the fit, only the position of the support vectors matter; any points further from the margin which are on the correct side do not modify the fit! Technically, this is because these points do not contribute to the loss function used to fit the model, so their position and number do not matter so long as they do not cross the margin.\nWe can see this, for example, if we plot the model learned from the first 60 points and first 120 points of this dataset:\n\ndef plot_svm(N=10, ax=None):\n    X, y = make_blobs(n_samples=200, centers=2,\n                      random_state=0, cluster_std=0.60)\n    X = X[:N]\n    y = y[:N]\n    model = SVC(kernel='linear', C=1E10)\n    model.fit(X, y)\n    \n    ax = ax or plt.gca()\n    ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n    ax.set_xlim(-1, 4)\n    ax.set_ylim(-1, 6)\n    plot_svc_decision_function(model, ax)\n\nfig, ax = plt.subplots(1, 2, figsize=(16, 6))\nfig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\nfor axi, N in zip(ax, [60, 120]):\n    plot_svm(N, axi)\n    axi.set_title('N = {0}'.format(N))\n\n\n\n\n\n\n\n\nIn the left panel, we see the model and the support vectors for 60 training points. In the right panel, we have doubled the number of training points, but the model has not changed: the three support vectors from the left panel are still the support vectors from the right panel. This insensitivity to the exact behavior of distant points is one of the strengths of the SVM model.\nIf you are running this notebook live, you can use IPython’s interactive widgets to view this feature of the SVM model interactively:\n\nfrom ipywidgets import interact, fixed\ninteract(plot_svm, N=[10, 200], ax=fixed(None));\n\n\n\n\n\n\n\n\n\n\nBeyond linear boundaries: Kernel SVM\nWhere SVM becomes extremely powerful is when it is combined with kernels. We have seen a version of kernels before, in the basis function regressions of In Depth: Linear Regression. There we projected our data into higher-dimensional space defined by polynomials and Gaussian basis functions, and thereby were able to fit for nonlinear relationships with a linear classifier.\nIn SVM models, we can use a version of the same idea. To motivate the need for kernels, let’s look at some data that is not linearly separable:\n\nfrom sklearn.datasets.samples_generator import make_circles\nX, y = make_circles(100, factor=.1, noise=.1)\n\nclf = SVC(kernel='linear').fit(X, y)\n\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\nplot_svc_decision_function(clf, plot_support=False);\n\n\n\n\n\n\n\n\nIt is clear that no linear discrimination will ever be able to separate this data. But we can draw a lesson from the basis function regressions in In Depth: Linear Regression, and think about how we might project the data into a higher dimension such that a linear separator would be sufficient. For example, one simple projection we could use would be to compute a radial basis function centered on the middle clump:\n\nr = np.exp(-(X ** 2).sum(1))\n\nWe can visualize this extra data dimension using a three-dimensional plot—if you are running this notebook live, you will be able to use the sliders to rotate the plot:\n\nfrom mpl_toolkits import mplot3d\n\ndef plot_3D(elev=30, azim=30, X=X, y=y):\n    ax = plt.subplot(projection='3d')\n    ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap='autumn')\n    ax.view_init(elev=elev, azim=azim)\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_zlabel('r')\n\ninteract(plot_3D, elev=[-90, 90], azip=(-180, 180),\n         X=fixed(X), y=fixed(y));\n\n\n\n\n\n\n\n\nWe can see that with this additional dimension, the data becomes trivially linearly separable, by drawing a separating plane at, say, r=0.7.\nHere we had to choose and carefully tune our projection: if we had not centered our radial basis function in the right location, we would not have seen such clean, linearly separable results. In general, the need to make such a choice is a problem: we would like to somehow automatically find the best basis functions to use.\nOne strategy to this end is to compute a basis function centered at every point in the dataset, and let the SVM algorithm sift through the results. This type of basis function transformation is known as a kernel transformation, as it is based on a similarity relationship (or kernel) between each pair of points.\nA potential problem with this strategy—projecting \\(N\\) points into \\(N\\) dimensions—is that it might become very computationally intensive as \\(N\\) grows large. However, because of a neat little procedure known as the kernel trick, a fit on kernel-transformed data can be done implicitly—that is, without ever building the full \\(N\\)-dimensional representation of the kernel projection! This kernel trick is built into the SVM, and is one of the reasons the method is so powerful.\nIn Scikit-Learn, we can apply kernelized SVM simply by changing our linear kernel to an RBF (radial basis function) kernel, using the kernel model hyperparameter:\n\nclf = SVC(kernel='rbf', C=1E6)\nclf.fit(X, y)\n\nSVC(C=1000000.0, cache_size=200, class_weight=None, coef0=0.0,\n  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n  max_iter=-1, probability=False, random_state=None, shrinking=True,\n  tol=0.001, verbose=False)\n\n\n\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\nplot_svc_decision_function(clf)\nplt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n            s=300, lw=1, facecolors='none');\n\n\n\n\n\n\n\n\nUsing this kernelized support vector machine, we learn a suitable nonlinear decision boundary. This kernel transformation strategy is used often in machine learning to turn fast linear methods into fast nonlinear methods, especially for models in which the kernel trick can be used.\n\n\nTuning the SVM: Softening Margins\nOur discussion thus far has centered around very clean datasets, in which a perfect decision boundary exists. But what if your data has some amount of overlap? For example, you may have data like this:\n\nX, y = make_blobs(n_samples=100, centers=2,\n                  random_state=0, cluster_std=1.2)\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');\n\n\n\n\n\n\n\n\nTo handle this case, the SVM implementation has a bit of a fudge-factor which “softens” the margin: that is, it allows some of the points to creep into the margin if that allows a better fit. The hardness of the margin is controlled by a tuning parameter, most often known as \\(C\\). For very large \\(C\\), the margin is hard, and points cannot lie in it. For smaller \\(C\\), the margin is softer, and can grow to encompass some points.\nThe plot shown below gives a visual picture of how a changing \\(C\\) parameter affects the final fit, via the softening of the margin:\n\nX, y = make_blobs(n_samples=100, centers=2,\n                  random_state=0, cluster_std=0.8)\n\nfig, ax = plt.subplots(1, 2, figsize=(16, 6))\nfig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n\nfor axi, C in zip(ax, [10.0, 0.1]):\n    model = SVC(kernel='linear', C=C).fit(X, y)\n    axi.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n    plot_svc_decision_function(model, axi)\n    axi.scatter(model.support_vectors_[:, 0],\n                model.support_vectors_[:, 1],\n                s=300, lw=1, facecolors='none');\n    axi.set_title('C = {0:.1f}'.format(C), size=14)\n\n\n\n\n\n\n\n\nThe optimal value of the \\(C\\) parameter will depend on your dataset, and should be tuned using cross-validation or a similar procedure (refer back to Hyperparameters and Model Validation).",
    "crumbs": [
      "语法格式",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>In-Depth: Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "body/05.07-Support-Vector-Machines.html#example-face-recognition",
    "href": "body/05.07-Support-Vector-Machines.html#example-face-recognition",
    "title": "In-Depth: Support Vector Machines",
    "section": "Example: Face Recognition",
    "text": "Example: Face Recognition\nAs an example of support vector machines in action, let’s take a look at the facial recognition problem. We will use the Labeled Faces in the Wild dataset, which consists of several thousand collated photos of various public figures. A fetcher for the dataset is built into Scikit-Learn:\n\nfrom sklearn.datasets import fetch_lfw_people\nfaces = fetch_lfw_people(min_faces_per_person=60)\nprint(faces.target_names)\nprint(faces.images.shape)\n\n['Ariel Sharon' 'Colin Powell' 'Donald Rumsfeld' 'George W Bush'\n 'Gerhard Schroeder' 'Hugo Chavez' 'Junichiro Koizumi' 'Tony Blair']\n(1348, 62, 47)\n\n\nLet’s plot a few of these faces to see what we’re working with:\n\nfig, ax = plt.subplots(3, 5)\nfor i, axi in enumerate(ax.flat):\n    axi.imshow(faces.images[i], cmap='bone')\n    axi.set(xticks=[], yticks=[],\n            xlabel=faces.target_names[faces.target[i]])\n\n\n\n\n\n\n\n\nEach image contains [62×47] or nearly 3,000 pixels. We could proceed by simply using each pixel value as a feature, but often it is more effective to use some sort of preprocessor to extract more meaningful features; here we will use a principal component analysis (see In Depth: Principal Component Analysis) to extract 150 fundamental components to feed into our support vector machine classifier. We can do this most straightforwardly by packaging the preprocessor and the classifier into a single pipeline:\n\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import RandomizedPCA\nfrom sklearn.pipeline import make_pipeline\n\npca = RandomizedPCA(n_components=150, whiten=True, random_state=42)\nsvc = SVC(kernel='rbf', class_weight='balanced')\nmodel = make_pipeline(pca, svc)\n\nFor the sake of testing our classifier output, we will split the data into a training and testing set:\n\nfrom sklearn.cross_validation import train_test_split\nXtrain, Xtest, ytrain, ytest = train_test_split(faces.data, faces.target,\n                                                random_state=42)\n\nFinally, we can use a grid search cross-validation to explore combinations of parameters. Here we will adjust C (which controls the margin hardness) and gamma (which controls the size of the radial basis function kernel), and determine the best model:\n\nfrom sklearn.grid_search import GridSearchCV\nparam_grid = {'svc__C': [1, 5, 10, 50],\n              'svc__gamma': [0.0001, 0.0005, 0.001, 0.005]}\ngrid = GridSearchCV(model, param_grid)\n\n%time grid.fit(Xtrain, ytrain)\nprint(grid.best_params_)\n\nCPU times: user 47.8 s, sys: 4.08 s, total: 51.8 s\nWall time: 26 s\n{'svc__gamma': 0.001, 'svc__C': 10}\n\n\nThe optimal values fall toward the middle of our grid; if they fell at the edges, we would want to expand the grid to make sure we have found the true optimum.\nNow with this cross-validated model, we can predict the labels for the test data, which the model has not yet seen:\n\nmodel = grid.best_estimator_\nyfit = model.predict(Xtest)\n\nLet’s take a look at a few of the test images along with their predicted values:\n\nfig, ax = plt.subplots(4, 6)\nfor i, axi in enumerate(ax.flat):\n    axi.imshow(Xtest[i].reshape(62, 47), cmap='bone')\n    axi.set(xticks=[], yticks=[])\n    axi.set_ylabel(faces.target_names[yfit[i]].split()[-1],\n                   color='black' if yfit[i] == ytest[i] else 'red')\nfig.suptitle('Predicted Names; Incorrect Labels in Red', size=14);\n\n\n\n\n\n\n\n\nOut of this small sample, our optimal estimator mislabeled only a single face (Bush’s face in the bottom row was mislabeled as Blair). We can get a better sense of our estimator’s performance using the classification report, which lists recovery statistics label by label:\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(ytest, yfit,\n                            target_names=faces.target_names))\n\n                   precision    recall  f1-score   support\n\n     Ariel Sharon       0.65      0.73      0.69        15\n     Colin Powell       0.81      0.87      0.84        68\n  Donald Rumsfeld       0.75      0.87      0.81        31\n    George W Bush       0.93      0.83      0.88       126\nGerhard Schroeder       0.86      0.78      0.82        23\n      Hugo Chavez       0.93      0.70      0.80        20\nJunichiro Koizumi       0.80      1.00      0.89        12\n       Tony Blair       0.83      0.93      0.88        42\n\n      avg / total       0.85      0.85      0.85       337\n\n\n\nWe might also display the confusion matrix between these classes:\n\nfrom sklearn.metrics import confusion_matrix\nmat = confusion_matrix(ytest, yfit)\nsns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n            xticklabels=faces.target_names,\n            yticklabels=faces.target_names)\nplt.xlabel('true label')\nplt.ylabel('predicted label');\n\n\n\n\n\n\n\n\nThis helps us get a sense of which labels are likely to be confused by the estimator.\nFor a real-world facial recognition task, in which the photos do not come pre-cropped into nice grids, the only difference in the facial classification scheme is the feature selection: you would need to use a more sophisticated algorithm to find the faces, and extract features that are independent of the pixellation. For this kind of application, one good option is to make use of OpenCV, which, among other things, includes pre-trained implementations of state-of-the-art feature extraction tools for images in general and faces in particular.",
    "crumbs": [
      "语法格式",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>In-Depth: Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "body/05.07-Support-Vector-Machines.html#support-vector-machine-summary",
    "href": "body/05.07-Support-Vector-Machines.html#support-vector-machine-summary",
    "title": "In-Depth: Support Vector Machines",
    "section": "Support Vector Machine Summary",
    "text": "Support Vector Machine Summary\nWe have seen here a brief intuitive introduction to the principals behind support vector machines. These methods are a powerful classification method for a number of reasons:\n\nTheir dependence on relatively few support vectors means that they are very compact models, and take up very little memory.\nOnce the model is trained, the prediction phase is very fast.\nBecause they are affected only by points near the margin, they work well with high-dimensional data—even data with more dimensions than samples, which is a challenging regime for other algorithms.\nTheir integration with kernel methods makes them very versatile, able to adapt to many types of data.\n\nHowever, SVMs have several disadvantages as well:\n\nThe scaling with the number of samples \\(N\\) is \\(\\mathcal{O}[N^3]\\) at worst, or \\(\\mathcal{O}[N^2]\\) for efficient implementations. For large numbers of training samples, this computational cost can be prohibitive.\nThe results are strongly dependent on a suitable choice for the softening parameter \\(C\\). This must be carefully chosen via cross-validation, which can be expensive as datasets grow in size.\nThe results do not have a direct probabilistic interpretation. This can be estimated via an internal cross-validation (see the probability parameter of SVC), but this extra estimation is costly.\n\nWith those traits in mind, I generally only turn to SVMs once other simpler, faster, and less tuning-intensive methods have been shown to be insufficient for my needs. Nevertheless, if you have the CPU cycles to commit to training and cross-validating an SVM on your data, the method can lead to excellent results.\n\n&lt; In Depth: Linear Regression | Contents | In-Depth: Decision Trees and Random Forests &gt;",
    "crumbs": [
      "语法格式",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>In-Depth: Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "body/TS_asset_return.html",
    "href": "body/TS_asset_return.html",
    "title": "1. 资产收益率（Asset Returns）",
    "section": "",
    "text": "Source: Tsay_2010_Analysis of financial time series-3rd.pdf, Chpater 1\n- Tsay, Ruey S. (2010). Analysis of Financial Time Series. 3rd Edition. Wiley.\n\n大多数金融研究侧重于资产的收益率而非价格。Campbell、Lo 和 MacKinlay（1997）指出了使用收益率的两个主要原因：\n\n对于普通投资者而言，资产的收益率是一种完整且无量纲的衡量指标；\n相比价格序列，收益率序列具有诸多良好的统计性质，更容易分析。\n\n然而，资产收益率的定义并不唯一，以下介绍几种常见定义（暂不考虑分红的情况）。\n\n1.1 单期简单收益率（One-Period Simple Return）\n若在时间点 \\(t - 1\\) 买入一项资产并持有至时间 \\(t\\)，其**简单总收益率（gross return）**定义为：\n\\[\n1 + R_t = \\frac{P_t}{P_{t-1}} \\quad \\text{或} \\quad P_t = P_{t-1}(1 + R_t)\n\\]\n则相应的 单期简单净收益率（net return） 为：\n\\[\nR_t = \\frac{P_t - P_{t-1}}{P_{t-1}}\n\\]\n\n\n1.2 多期简单收益率（Multiperiod Simple Return）\n若持有资产 \\(k\\) 个时期（从 \\(t - k\\) 到 \\(t\\)），则 \\(k\\) 期总简单收益率 为：\n\\[\n1 + R_t^{[k]} = \\frac{P_t}{P_{t-k}} = (1 + R_t)(1 + R_{t-1}) \\cdots (1 + R_{t-k+1}) = \\prod_{j=0}^{k-1} (1 + R_{t-j})\n\\]\n即为单期简单收益率的乘积，又称复利收益（compound return）。\n因此，\\(k\\) 期净简单收益率可以采用如下公式计算：\n\\[\nR_t^{[k]} = \\frac{P_t - P_{t-k}}{P_{t-k}}\n\\]\n在实际中，讨论收益率时需注意时间间隔（如月度、年度等）。若资产持有时间为 \\(k\\) 年，则年化收益率为：\n\\[\n\\text{Annualized} \\{R_t^{[k]}\\} = \\left( \\prod_{j=0}^{k-1} (1 + R_{t-j}) \\right)^{1/k} - 1\n\\]\n这可理解为几何平均收益率，其对数形式计算为：\n\\[\n\\text{Annualized} \\{R_t^{[k]}\\} = \\exp \\left( \\frac{1}{k} \\sum_{j=0}^{k-1} \\ln(1 + R_{t-j}) \\right) - 1\n\\]\n由于一阶泰勒展开较简单，且当 \\(R_t\\) 较小时近似较好，因此可采用以下近似公式：\n\\[\n\\text{Annualized} \\{R_t^{[k]}\\} \\approx \\frac{1}{k} \\sum_{j=0}^{k-1} R_{t-j}\n\\]\n但在某些应用中 (部分时点上的收益率过高)，该近似的准确性可能不足。\n\n\n1.3 连续复利收益率（Continuously Compounded Return）\n在介绍连续复利收益率之前，先理解复利的影响。假设年利率为 10%，初始投资为 $1：\n\n若年付一次，则最终价值为：$1 × (1 + 0.1) = $1.10\n若半年一次，单次利率为 5%，则一年后为：$1 × (1 + 0.05)² = $1.1025\n若按 \\(m\\) 次支付，则终值 (FV)：\n\n\\[\nFV = 1 × \\left(1 + \\frac{0.1}{m} \\right)^m\n\\]\n当 \\(m \\to \\infty\\) 时，终值趋近于 \\(\\exp(0.1) = 1.10517\\)，即连续复利的结果。\n因此，连续复利终值为：\n\\[\nA = C \\exp(r × n)\n\\]\n其中 \\(C\\) 为初始资金，\\(r\\) 为年利率，\\(n\\) 为投资年数。其对应的现值计算公式为：\n\\[\nC = A \\exp(-r × n)\n\\]\n\n\n1.4 对数收益率（Log Return）\n资产的对数收益率定义为简单收益率总额的自然对数：\n\\[\nr_t = \\ln(1 + R_t) = \\ln \\left(\\frac{P_t}{P_{t-1}} \\right) = \\ln(P_t) - \\ln(P_{t-1}) = p_t - p_{t-1}\n\\]\n其中，\\(p_t = \\ln(P_t)\\)。因此，对数收益率也称为对数价格差（log price difference），通常表示为 \\(r_t = \\Delta\\ln P_t\\)。\n\n补充说明：\n在实际分析中，常常对价格序列取自然对数后再进行一阶差分。对于较小的收益率，变量对数的一阶差分可以近似为该变量的百分比变化，即简单收益率：\n\n\\[\n\\begin{aligned}\n\\Delta \\ln(y_t) &= \\ln(y_t) - \\ln(y_{t-1}) \\\\\n&= \\ln\\left(\\frac{y_t}{y_{t-1}}\\right) \\\\\n&= \\ln(1 + R_t) \\\\\n&\\approx R_t = \\frac{y_t - y_{t-1}}{y_{t-1}}\n\\end{aligned}\n\\]\n其中 \\(R_t\\) 表示 \\(t\\) 时点的简单收益率。这里利用了 \\(\\ln(1 + x) \\approx x\\) 当 \\(x\\) 较小时的近似关系。\n对数收益率的优点：\n\n多期总对数收益率是各期对数收益率之和：\n\n\\[\nr_t^{[k]} = r_t + r_{t-1} + \\cdots + r_{t-k+1}\n\\]\n\n统计分析更为方便，尤其在建模时更具可操作性。\n\n\n\n1.5 投资组合收益（Portfolio Return）\n若一个投资组合包含 \\(N\\) 个资产，权重为 \\(w_i\\)，则其简单净收益为：\n\\[\nR_{p,t} = \\sum_{i=1}^{N} w_i R_{it}\n\\]\n但对数收益率不满足线性加权性质。若 \\(R_{it}\\) 较小，可以用近似公式：\n\\[\nr_{p,t} \\approx \\sum_{i=1}^{N} w_i r_{it}\n\\]\n\n\n1.6 分红影响（Dividend Payment）\n若资产在 \\(t - 1\\) 至 \\(t\\) 期间支付红利 \\(D_t\\)，则应调整收益率计算：\n\\[\nR_t = \\frac{P_t + D_t}{P_{t-1}} - 1, \\quad r_t = \\ln(P_t + D_t) - \\ln(P_{t-1})\n\\]\n\n\n1.7 超额收益（Excess Return）\n超额收益为资产收益与某一基准（如短期国债）收益之间的差值：\n\\[\nZ_t = R_t - R_{0t}, \\quad z_t = r_t - r_{0t}\n\\]\n这是套利组合的理论回报：做多资产、做空基准，无初始投资。\n\n注：“做多”即拥有该资产；“做空”指借入资产出售，并在未来买回偿还。做空者需支付红利给持有人，因此资产下跌才能盈利。\n\n\n\n小结\n\n对数与简单收益率关系：\n\n\\[\nr_t = \\ln(1 + R_t), \\quad R_t = e^{r_t} - 1\n\\]\n  若用百分数表达：\n\\[\nr_t = 100 \\ln\\left(1 + \\frac{R_t}{100} \\right), \\quad R_t = 100\\left(e^{r_t / 100} - 1 \\right)\n\\]\n\n多期收益率聚合关系：\n\n\\[\n1 + R_t^{[k]} = \\prod_{j=0}^{k-1}(1 + R_{t-j}), \\quad r_t^{[k]} = \\sum_{j=0}^{k-1} r_{t-j}\n\\]\n\n现值与终值关系（连续复利）：\n\n\\[\nA = C \\exp(r × n), \\quad C = A \\exp(-r × n)\n\\]",
    "crumbs": [
      "金融数据分析",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>1. 资产收益率（Asset Returns）</span>"
    ]
  },
  {
    "objectID": "body/TS_FRED_US_unemploy_rate.html",
    "href": "body/TS_FRED_US_unemploy_rate.html",
    "title": "获取宏观数据",
    "section": "",
    "text": "下载美国失业率数据\n本讲以 pandas_datareader 为例，介绍如何获取宏观数据。\nFRED 是美国联邦储备银行提供的宏观经济数据，包含了大量的经济指标。 FRED 提供了一个 API 接口，可以通过 pandas_datareader 来获取数据。\n本文写作过程中借助了 AI，包括 ChatGPT (提示词) 和 Github Coplilot。\n# 基本设定\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pandas_datareader.data import DataReader\nimport datetime\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")  # 屏蔽警告信息\n\n# 设置起止日期 (后续其他宏观变量也采用这个设置)\nstart_date = datetime.datetime(1960, 1, 1)\nend_date = datetime.datetime.today()\nstart_year = start_date.year\n# 失业率\n\n# 从 FRED 获取“Unemployment Rate”数据（代码为 UNRATE）\ndf_unemp = DataReader(\"UNRATE\", \"fred\", start_date, end_date)\n\n# 绘图\nplt.figure(figsize=(12, 6))\nplt.plot(df_unemp.index, df_unemp[\"UNRATE\"], \n         color='blue', linewidth=2, label='Unemployment Rate')\n\n# 图形美化\nplt.title(f\"Unemployment Rate in the U.S. ({start_year} - Present)\", fontsize=16)\nplt.xlabel(\"Date\", fontsize=12)\nplt.ylabel(\"Percent\", fontsize=12)\nplt.grid(True, linestyle='--', linewidth=0.5)\nplt.legend()\nplt.tight_layout()\n\n# 显示图形\nplt.show()",
    "crumbs": [
      "金融数据分析",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>获取宏观数据</span>"
    ]
  },
  {
    "objectID": "body/TS_FRED_US_unemploy_rate.html#下载美国失业率数据",
    "href": "body/TS_FRED_US_unemploy_rate.html#下载美国失业率数据",
    "title": "获取宏观数据",
    "section": "",
    "text": "Source: U.S. Bureau of Labor Statistics, Unemployment Rate [UNRATE], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/UNRATE, May 5, 2025.",
    "crumbs": [
      "金融数据分析",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>获取宏观数据</span>"
    ]
  },
  {
    "objectID": "body/TS_FRED_US_unemploy_rate.html#美国失业率的时序特征",
    "href": "body/TS_FRED_US_unemploy_rate.html#美国失业率的时序特征",
    "title": "获取宏观数据",
    "section": "美国失业率的时序特征",
    "text": "美国失业率的时序特征\n上图展示了自 1960 年以来美国的月度失业率变化趋势。总体来看，美国失业率呈现出显著的周期性波动，其高峰通常与经济衰退期相吻合，低谷则出现在经济扩张阶段。\n\n周期波动明显：失业率大致每 8-10 年出现一次较大波动，与美国历次经济衰退（如 1974、1982、1991、2008 和 2020 年）高度对应。\n历史极值：2020 年新冠疫情爆发初期，失业率迅速飙升至超过 14%，为图中最高点，反映出突发公共卫生事件对劳动市场的巨大冲击。\n长期下行趋势：尽管存在周期性波动，但在部分阶段（如 1982-2000 年间、2010-2019 年间）可观察到失业率逐步下降的趋势，显示出结构性改善可能性。\n\n\n问题：如何分析失业率的时序特征？\n从图中失业率的走势出发，我们可以从以下几个角度提出计量建模中需要关注的核心问题：\n\n平稳性与周期性： 失业率呈现一定的均值回复特征 (在上图中，均值约为 6Z%)，但是否真正平稳？如何通过单位根检验（如 ADF 检验）判断？如果该序列非平稳，我们是否应进行差分处理以便后续建模？\n结构性突变： 例如 2020 年的断崖式上升显然并非常态波动，这提示我们应考虑模型中可能存在的结构突变（structural break）。在传统 ARIMA 模型之外，我们可能需要引入 regime-switching 或 dummy 变量来捕捉这类异质性。\n滞后依赖结构： 当前失业率是否受到过去若干期值的影响？其滞后项在建模中如何体现？这正是 AR(p) 或 ARMA(p, q) 模型关注的核心。\n长期关系与协整： 若将失业率与其他宏观变量（如通货膨胀率、GDP 增长率）联合考虑，它们之间是否存在协整关系？若存在，应如何建立误差修正模型（ECM）？\n波动性建模： 某些阶段的波动显著大于其他时期，例如 1970s 或 2008 危机期间。如何刻画这种条件异方差特征？这将引出 ARCH/GARCH 及其扩展模型的讨论。\n\n\n# 失业率的基本统计特征\numemp = df_unemp[\"UNRATE\"]\nprint(\"失业率的基本统计特征：\")\nprint(umemp.describe().round(2))\n\n失业率的基本统计特征：\ncount    784.00\nmean       5.88\nstd        1.70\nmin        3.40\n25%        4.60\n50%        5.60\n75%        7.00\nmax       14.80\nName: UNRATE, dtype: float64\n\n\n\n# 分时段统计 (表格版)\nperiods = {\n    \"1960-1980年\": (\"1960-01-01\", \"1980-12-31\"),\n    \"1980-2000年\": (\"1980-01-01\", \"2000-12-31\"),\n    \"2000-2010年\": (\"2000-01-01\", \"2010-12-31\"),\n    \"2010-2020年\": (\"2010-01-01\", \"2020-12-31\"),\n    \"2020-2025年\": (\"2020-01-01\", \"2025-12-31\"),\n}\n\n# 创建一个列表来存储结果\nstats_list = []\n\nfor period, (start, end) in periods.items():\n    stats = umemp[start:end].agg(['mean', 'std', 'min', 'max']).round(2)\n    stats_list.append({\n        \"Period\": period,\n        \"Mean\": stats[\"mean\"],\n         \"Std\": stats[\"std\"],\n         \"Min\": stats[\"min\"],\n         \"Max\": stats[\"max\"]\n    })\n\n# 将结果转换为 DataFrame\nstats_table = pd.DataFrame(stats_list)\n\n# 显示结果表格\nprint(stats_table)\n\n       Period  Mean   Std  Min   Max\n0  1960-1980年  5.58  1.35  3.4   9.0\n1  1980-2000年  6.40  1.55  3.8  10.8\n2  2000-2010年  5.91  1.81  3.8  10.0\n3  2010-2020年  6.39  2.29  3.5  14.8\n4  2020-2025年  4.90  2.28  3.4  14.8",
    "crumbs": [
      "金融数据分析",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>获取宏观数据</span>"
    ]
  },
  {
    "objectID": "body/TS_FRED_US_unemploy_rate.html#年移动平均",
    "href": "body/TS_FRED_US_unemploy_rate.html#年移动平均",
    "title": "获取宏观数据",
    "section": "10 年移动平均",
    "text": "10 年移动平均\n我们可以使用 rolling 函数来计算 10 年移动平均，以便从更长时间尺度上观察失业率的变化趋势。\n\n# 10 年和 5 年滚动平均\nrolling_mean_umemp_10yr = umemp.rolling(window=120).mean()  # 120 个月 = 10 年\n\n# 绘图\nplt.figure(figsize=(12, 6))\nplt.plot(df_unemp.index, umemp, \n         color='blue', linewidth=2, label='Unemployment Rate')\nplt.plot(df_unemp.index, rolling_mean_umemp_10yr, \n         color='red', linewidth=4, label='10-Year Rolling Mean')\nplt.legend() # 添加图例\nplt.grid(True, linestyle='--', linewidth=0.5) # 添加网格线\n\n\n\n\n\n\n\n\n\n\\(AR(p)\\) 模型\n在时间序列分析中，\\(AR(p)\\) 模型是最基本的模型之一。它假设当前值与过去 \\(p\\) 个时刻的值存在线性关系。一般形式为：\n\\[\nX_t = \\phi_1 X_{t-1} + \\phi_2 X_{t-2} + ... + \\phi_p X_{t-p} + \\epsilon_t \\tag{1}\n\\]\n其中，\\(\\phi_1, \\phi_2, ..., \\phi_p\\) 是模型参数，\\(\\epsilon_t\\) 是白噪声项。\n\n\\(AR(1)\\) 模型\n当 \\(p=1\\) 时，\\(AR(1)\\) 模型为：\n\\[\nX_t = \\phi_1 X_{t-1} + \\epsilon_t \\tag{2}\n\\]\n虽然看起来很简单，但 \\(AR(1)\\) 模型在时间序列分析中非常重要，因为它可以捕捉到数据的自相关性。从模型设定形式上来看，它具有递推的特征，当前值仅与前一个值相关。\n具体而言，(2) 式在 \\(t-1\\) 时刻可以表示为：\n\\[\nX_{t-1} = \\phi_1 X_{t-2} + \\epsilon_{t-1} \\tag{3}\n\\]\n将 (3) 式代入 (2) 式中，我们可以得到：\n\\[\nX_t = \\phi_1 (\\phi_1 X_{t-2} + \\epsilon_{t-1}) + \\epsilon_t = \\phi_1^2 X_{t-2} + \\phi_1 \\epsilon_{t-1} + \\epsilon_t \\tag{4}\n\\]\n将 (4) 式继续递推下去，我们可以得到：\n\\[\nX_t = \\phi_1^t X_0 + \\sum_{i=0}^{t-1} \\phi_1^i \\epsilon_{t-i} \\tag{5}\n\\]\n\n式表明，\\(X_t\\) 由初始值 \\(X_0\\) 和过去的随机扰动项 \\(\\epsilon_{t-i}\\) 线性组合而成。\n\n举个例子：若 \\(X_0\\) 表示某人 20 岁时的体重，取 \\(t = 10\\)，则 \\(X_{10}\\) 表示该人 30 岁时的体重。假设 \\(X_0 = 60 kg\\)，\\(\\phi_1 = 0.9\\)，则 (5) 式的含义是：\n\n该人 30 岁时的体重 \\(X_{10}\\)，由其 20 岁时的体重 \\(X_0\\) 与过去 10 年中的随机扰动项 \\(\\varepsilon_{t-i}\\)（如饮食习惯、运动频率、作息变化、疾病史等）线性加权而成。\n由于 \\(\\phi_1 = 0.9\\)，而 \\(\\phi_1^{10} = 0.9^{10} \\approx 0.35\\)，说明 20 岁时的初始体重对 30 岁时体重的影响仍然存在，但已显著减弱。体重的变化更多地取决于过去 10 年逐年积累的生活方式等随机因素。\n模型还体现出“记忆衰减”效应：越接近当前年份的扰动项，其影响越大。例如，\\(\\varepsilon_9\\) 的系数为 \\(\\phi_1^1 = 0.9\\)，\\(\\varepsilon_8\\) 的系数为 \\(\\phi_1^2 = 0.81\\)，\\(\\varepsilon_7\\) 的系数为 \\(\\phi_1^3 \\approx 0.729\\)，依此类推，扰动项的影响呈指数递减。\n\n下面，我们来模拟生成几组 \\(AR(1)\\) 过程的数据。\n请思考如下几个问题： - \\(AR(1)\\) 模型能刻画一个人的体重变化吗？ - 不同的 \\(\\phi_1\\) 对数据生成有何影响？ - 模拟过程中，假设 \\(\\epsilon_t \\sim (0, \\sigma^2)\\)，参数 \\(\\sigma\\) 的取值对数据生成有何影响？ - 如何修改这个模型才能更好地刻画一个人的体重变化？\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 参数设置\nphi_1 = 0.99  # AR(1) 系数\nsigma = 5  # 随机扰动项的标准差\n\nX_0 = 60      # 初始体重 (kg)\nn_years = 10 # 模拟 10 年\nn_steps = n_years * 12  # 每年 12 个月\n\n# 随机扰动项 (白噪声)\n#np.random.seed(42)  # 固定随机种子以便复现\nepsilon = np.random.normal(loc=0, scale=sigma, size=n_steps)\n\n# 模拟 AR(1) 过程\nX = np.zeros(n_steps)\nX[0] = X_0\nfor t in range(1, n_steps):\n    X[t] = phi_1 * X[t-1] + epsilon[t]\n\n# 绘图\ntime = np.arange(1, n_steps + 1) / 12  # 时间轴 (以年为单位)\nplt.figure(figsize=(12, 6))\nplt.plot(time, X, label=\"Simulated Weight (kg)\", color=\"blue\", linewidth=2)\nplt.axhline(y=X_0, color=\"red\", linestyle=\"--\", label=\"Initial Weight (3 kg)\")\nplt.title(\"Simulated Weight Over 30 Years (AR(1) Process)\", fontsize=16)\nplt.xlabel(\"Time (Years)\", fontsize=12)\nplt.ylabel(\"Weight (kg)\", fontsize=12)\nplt.legend()\nplt.grid(True, linestyle=\"--\", linewidth=0.5)\nplt.show()\n\n\n\n\n\n\n\n\n\n# CPI（消费者物价指数）\n\n# 从 FRED 获取 CPI 数据（代码为 CPIAUCNS）\ndf_CPI = DataReader(\"CPIAUCNS\", \"fred\", start_date, end_date)\n\n# 绘图\nplt.figure(figsize=(12, 6))\nplt.plot(df_CPI.index, df_CPI[\"CPIAUCNS\"], \n         color='darkred', linewidth=2, label='CPI: All Urban Consumers')\n\n# 图形美化\nplt.title(f\"Consumer Price Index (CPI-U): All Items in U.S. City Average ({start_year} - Present)\", fontsize=16)\nplt.xlabel(\"Date\", fontsize=12)\nplt.ylabel(\"Index (1982-1984 = 100)\", fontsize=12)\nplt.grid(True, linestyle='--', linewidth=0.5)\nplt.legend()\nplt.tight_layout()\n\n# 显示图形\nplt.show()\n\n\n\n\n\n\n\n\n\n# 基于 CPI 计算通货膨胀率\n\n# 计算通货膨胀率（基于 CPI 的同比变化率）\ndf_CPI['Inflation Rate'] = df_CPI['CPIAUCNS'].pct_change(periods=12) * 100\n\n# 绘图\nplt.figure(figsize=(12, 6))\nplt.plot(df_CPI.index, df_CPI['Inflation Rate'], \n         color='green', linewidth=2, label='Inflation Rate')\n\n# 图形美化\nplt.title(f\"Inflation Rate in the U.S. ({start_year} - Present)\", fontsize=16)\nplt.xlabel(\"Date\", fontsize=12)\nplt.ylabel(\"Inflation Rate (%)\", fontsize=12)\nplt.grid(True, linestyle='--', linewidth=0.5)\nplt.legend()\nplt.tight_layout()\n\n# 显示图形\nplt.show()\n\n\n\n\n\n\n\n\n\n# 绘图：失业率 + 通货膨胀率\nplt.figure(figsize=(12, 6))\n\n# 绘制失业率\nplt.plot(df_unemp.index, df_unemp[\"UNRATE\"], \n         color='blue', linewidth=2, label='Unemployment Rate')\n\n# 绘制通货膨胀率\nplt.plot(df_CPI.index, df_CPI['Inflation Rate'], \n         color='green', linewidth=2, label='Inflation Rate')\n\n# 添加图例\nplt.legend(loc='upper left')\n\n# 添加数据来源说明\nplt.figtext(0.5, -0.05, \"Source: U.S. Bureau of Labor Statistics, FRED (https://fred.stlouisfed.org/)\", \n            wrap=True, horizontalalignment='center', fontsize=10)\n\nText(0.5, -0.05, 'Source: U.S. Bureau of Labor Statistics, FRED (https://fred.stlouisfed.org/)')\n\n\n\n\n\n\n\n\n\n这幅图展示了 1960 年至今美国 失业率（Unemployment Rate） 与 通胀率（Inflation Rate） 的时间序列走势。可以从以下几个方面进行解读：\n\n趋势与波动性差异：\n\n失业率（蓝线）表现出相对平稳的周期性波动，具有一定的周期长度；\n通胀率（绿线）波动幅度更大，尤其在 1970s 至 1980s 初期，有显著的尖峰。\n\n结构性事件的影响：\n\n1970s：石油危机期间，通胀迅速上升至 13% 以上（即“滞涨”现象），而失业率也居高不下；\n2008 金融危机：失业率大幅上升，而通胀保持较低水平；\n2020 疫情冲击：失业率暴涨，通胀短暂回落，随后于 2021-2022 再次激增。\n\n负相关性阶段：\n\n在部分阶段（例如 1980s 中后期、1990s、2010s），通胀与失业呈现出某种程度的负相关，符合传统的菲利普斯曲线（Phillips Curve）设定。",
    "crumbs": [
      "金融数据分析",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>获取宏观数据</span>"
    ]
  },
  {
    "objectID": "body/TS_FRED_US_unemploy_rate.html#建模思路",
    "href": "body/TS_FRED_US_unemploy_rate.html#建模思路",
    "title": "获取宏观数据",
    "section": "建模思路",
    "text": "建模思路\n如果我们想建立一个模型来刻画失业率与通胀率之间的关系，有哪些可行的建模思路呢？\n\n# UNRATE 和 Inflation Rate 的关系\n\n## 相关系数\ncorrelation = df_unemp[\"UNRATE\"].corr(df_CPI['Inflation Rate'])\nprint(f\"Correlation between Unemployment Rate and Inflation Rate: {correlation:.2f}\")\n\n## OLS 回归分析\nimport statsmodels.api as sm\n# Align the indices of X and Y\nX = df_CPI['Inflation Rate'].dropna()   # 自变量\nY = df_unemp[\"UNRATE\"]                  # 因变量\nX, Y = X.align(Y, join='inner')         # Align indices\nX = sm.add_constant(X)                  # 添加常数项\nmodel = sm.OLS(Y, X).fit()              # OLS 回归\nmodel_summary = model.summary()       # 回归结果\nprint(model_summary)\n\nCorrelation between Unemployment Rate and Inflation Rate: 0.06\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 UNRATE   R-squared:                       0.004\nModel:                            OLS   Adj. R-squared:                  0.003\nMethod:                 Least Squares   F-statistic:                     3.016\nDate:                Tue, 06 May 2025   Prob (F-statistic):             0.0829\nTime:                        00:52:40   Log-Likelihood:                -1505.1\nNo. Observations:                 771   AIC:                             3014.\nDf Residuals:                     769   BIC:                             3024.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==================================================================================\n                     coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------\nconst              5.7423      0.103     55.657      0.000       5.540       5.945\nInflation Rate     0.0380      0.022      1.737      0.083      -0.005       0.081\n==============================================================================\nOmnibus:                      104.203   Durbin-Watson:                   0.065\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              155.789\nSkew:                           0.926   Prob(JB):                     1.48e-34\nKurtosis:                       4.190   Cond. No.                         8.16\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n# ARMA (1,1) 模型：失业率\nimport statsmodels.api as sm\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.stattools import adfuller\n\nmodel_unemp = ARIMA(df_unemp['UNRATE'].dropna(), order=(1, 0, 1))\nmodel_unemp_fit = model_unemp.fit()  # Fit the ARIMA model\nprint(model_unemp_fit.summary())  # Print the model summary\n\n                               SARIMAX Results                                \n==============================================================================\nDep. Variable:                 UNRATE   No. Observations:                  784\nModel:                 ARIMA(1, 0, 1)   Log Likelihood                -446.406\nDate:                Tue, 06 May 2025   AIC                            900.812\nTime:                        00:54:53   BIC                            919.469\nSample:                    01-01-1960   HQIC                           907.986\n                         - 04-01-2025                                         \nCovariance Type:                  opg                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          5.8040      1.075      5.397      0.000       3.696       7.912\nar.L1          0.9640      0.012     82.124      0.000       0.941       0.987\nma.L1          0.0508      0.012      4.417      0.000       0.028       0.073\nsigma2         0.1822      0.002     94.889      0.000       0.178       0.186\n===================================================================================\nLjung-Box (L1) (Q):                   0.00   Jarque-Bera (JB):           6130464.69\nProb(Q):                              0.96   Prob(JB):                         0.00\nHeteroskedasticity (H):              11.77   Skew:                            17.73\nProb(H) (two-sided):                  0.00   Kurtosis:                       434.75\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n\n\n\n# ARMA (1,1) 模型：通胀率\n\nmodel = ARIMA(df_CPI['Inflation Rate'].dropna(), order=(1, 0, 1))\nmodel_fit = model.fit()     # Fit the ARIMA model\nprint(model_fit.summary())  # Print the model summary\n\n                               SARIMAX Results                                \n==============================================================================\nDep. Variable:         Inflation Rate   No. Observations:                  771\nModel:                 ARIMA(1, 0, 1)   Log Likelihood                -327.515\nDate:                Tue, 06 May 2025   AIC                            663.030\nTime:                        00:55:03   BIC                            681.621\nSample:                    01-01-1961   HQIC                           670.185\n                         - 03-01-2025                                         \nCovariance Type:                  opg                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          3.5391      1.139      3.108      0.002       1.307       5.771\nar.L1          0.9847      0.005    197.487      0.000       0.975       0.995\nma.L1          0.2964      0.026     11.248      0.000       0.245       0.348\nsigma2         0.1362      0.004     31.231      0.000       0.128       0.145\n===================================================================================\nLjung-Box (L1) (Q):                   2.49   Jarque-Bera (JB):               406.03\nProb(Q):                              0.11   Prob(JB):                         0.00\nHeteroskedasticity (H):               1.14   Skew:                            -0.16\nProb(H) (two-sided):                  0.28   Kurtosis:                         6.54\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n\n\n\n单位根检验\n从上面的 \\(ARMA(1,1)\\) 模型的结果来看，\\(AR(1)\\) 系数的估计值为 \\(0.9847\\)，接近于 \\(1\\)，这表明该序列可能是一个单位根序列。\n我们可以使用 statsmodels 库中的 adfuller 函数来进行单位根检验。\n\nADF 检验\n给定一个时间序列 \\(X_t\\)，我们可以使用以下的 ADF 检验来检验 \\(X_t\\) 是否是平稳的： \\[\nX_t = \\phi_0 + \\phi_1 X_{t-1} + \\phi_2 X_{t-2} + ... + \\phi_p X_{t-p} + \\epsilon_t\\] 其中，\\(\\epsilon_t\\) 是一个白噪声序列。\nADF 检验的原假设是：\\(X_t\\) 是一个单位根序列，即 \\(H_0: \\phi_1 = 1\\)。 如果 \\(H_0\\) 被拒绝，则说明 \\(X_t\\) 是平稳的。\nADF 检验包含几种典型的数据生成机制： - 纯随机游走：\\(X_t = X_{t-1} + \\epsilon_t\\)，其中 \\(\\epsilon_t\\) 是一个白噪声序列。 - 随机游走加趋势：\\(X_t = \\phi_0 + \\phi_1 X_{t-1} + \\phi_2 t + \\epsilon_t\\)，其中 \\(\\epsilon_t\\) 是一个白噪声序列，\\(t\\) 是时间趋势项。 - 随机游走加季节性：\\(X_t = \\phi_0 + \\phi_1 X_{t-1} + S_t + \\epsilon_t\\)，其中 \\(\\epsilon_t\\) 是一个白噪声序列，\\(S_t\\) 是季节性项。 - 随机游走加趋势和季节性：\\(X_t = \\phi_0 + \\phi_1 X_{t-1} + \\phi_2 t + S_t + \\epsilon_t\\)，其中 \\(\\epsilon_t\\) 是一个白噪声序列，\\(t\\) 是时间趋势项，\\(S_t\\) 是季节性项。\n\n\nKPSS 检验\nKPSS 检验的原假设是：\\(X_t\\) 是平稳的，即 \\(H_0: \\phi_1 &lt; 1\\)。 如果 \\(H_0\\) 被拒绝，则说明 \\(X_t\\) 是一个单位根序列。\n\n\nPP 检验\nPP 检验的原假设是：\\(X_t\\) 是一个单位根序列，即 \\(H_0: \\phi_1 = 1\\)。 如果 \\(H_0\\) 被拒绝，则说明 \\(X_t\\) 是平稳的。\n\n\n对比\n\n# 单位根检验\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.stattools import kpss\n\n# ADF 检验\nadf_result = adfuller(df_CPI['Inflation Rate'].dropna())\nprint(f\"ADF Statistic: {adf_result[0]:.4f}\")\nprint(f\"p-value: {adf_result[1]:.4f}\")\nprint(f\"Critical Values: {adf_result[4]}\")\n\n# KPSS 检验\nkpss_result = kpss(df_CPI['Inflation Rate'].dropna(), regression='c')\nprint(f\"KPSS Statistic: {kpss_result[0]:.4f}\")\nprint(f\"p-value: {kpss_result[1]:.4f}\")\n\n\nADF Statistic: -3.2255\np-value: 0.0186\nCritical Values: {'1%': -3.4390409569041207, '5%': -2.865375732701395, '10%': -2.568812543748081}\nKPSS Statistic: 0.8623\np-value: 0.0100\n\n\n\n# 绘制 ACF 和 PACF 图\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nplt.figure(figsize=(12, 6))\nplt.subplot(2, 1, 1)\nplot_acf(df_CPI['Inflation Rate'].dropna(), lags=40, ax=plt.gca())\nplt.title('ACF of Inflation Rate', fontsize=16)\nplt.subplot(2, 1, 2)\nplot_pacf(df_CPI['Inflation Rate'].dropna(), lags=40, ax=plt.gca())\nplt.title('PACF of Inflation Rate', fontsize=16)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n解读： 上面的图形展示了美国失业率和通货膨胀率的时间序列变化趋势。可以观察到，失业率和通货膨胀率在不同时间段内呈现出一定的波动性。失业率在经济衰退期间通常会显著上升，而通货膨胀率则可能受到多种因素的影响，包括货币政策、供需变化等。通过对比两条曲线，可以进一步分析它们之间的关系，例如是否存在菲利普斯曲线的特征。此外，结合回归分析和相关系数的计算结果，可以定量评估失业率与通货膨胀率之间的相关性。\n\n\n## VAR 模型\nfrom statsmodels.tsa.api import VAR\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.stattools import grangercausalitytests\nfrom statsmodels.tsa.stattools import coint\nfrom statsmodels.tsa.stattools import adfuller, kpss\nimport statsmodels.api as sm\nimport numpy as np\nimport statsmodels.formula.api as smf\nimport statsmodels.tsa.stattools as ts\n\n# 1. 检验平稳性\ndef test_stationarity(timeseries):\n    # 进行ADF检验\n    adf_result = adfuller(timeseries)\n    print(f'ADF Statistic: {adf_result[0]}')\n    print(f'p-value: {adf_result[1]}')\n    if adf_result[1] &lt;= 0.05:\n        print(\"Reject the null hypothesis: The time series is stationary.\")\n    else:\n        print(\"Fail to reject the null hypothesis: The time series is non-stationary.\")\n\n    # 进行KPSS检验\n    kpss_result = kpss(timeseries, regression='c')\n    print(f'KPSS Statistic: {kpss_result[0]}')\n    print(f'p-value: {kpss_result[1]}')\n    if kpss_result[1] &lt;= 0.05:\n        print(\"Reject the null hypothesis: The time series is non-stationary.\")\n    else:\n        print(\"Fail to reject the null hypothesis: The time series is stationary.\")\n\n# 2. 绘制自相关和偏自相关图\ndef plot_acf_pacf(timeseries):\n    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n    plot_acf(timeseries, lags=40, ax=ax[0])\n    plot_pacf(timeseries, lags=40, ax=ax[1])\n    plt.show()\n\n# 3. VAR模型\ndef fit_var_model(data, maxlags=15):\n    model = VAR(data)\n    results = model.fit(maxlags=maxlags, ic='aic')\n    print(results.summary())\n    return results\n\n# 4. Granger因果关系检验\ndef granger_causality_test(data, max_lag=15):\n    test_result = grangercausalitytests(data, max_lag, verbose=True)\n    return test_result\n\n# 5. 协整检验\ndef cointegration_test(data):\n    score, p_value, _ = coint(data.iloc[:, 0], data.iloc[:, 1])\n    print(f'Cointegration test statistic: {score}')\n    print(f'p-value: {p_value}')\n    if p_value &lt;= 0.05:\n        print(\"Reject the null hypothesis: The time series are cointegrated.\")\n    else:\n        print(\"Fail to reject the null hypothesis: The time series are not cointegrated.\")\n\n# 6. VAR模型的脉冲响应函数\ndef impulse_response_function(model, steps=10):\n    irf = model.irf(steps)\n    irf.plot(orth=False)\n    plt.show()\n\n# 7. VAR模型的方差分解\ndef variance_decomposition(model, steps=10):\n    fevd = model.fevd(steps)\n    fevd.plot()\n    plt.show()\n\n# 8. VAR模型的预测\ndef forecast_var_model(model, steps=10):\n    forecast = model.forecast(model.y, steps=steps)\n    forecast_df = pd.DataFrame(forecast, index=pd.date_range(start=df_unemp.index[-1] + pd.DateOffset(1), periods=steps, freq='M'), columns=model.names)\n    return forecast_df",
    "crumbs": [
      "金融数据分析",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>获取宏观数据</span>"
    ]
  },
  {
    "objectID": "body/TS_SZ_index.html",
    "href": "body/TS_SZ_index.html",
    "title": "时间序列分析：上证指数的时序特征",
    "section": "",
    "text": "交互图\n本讲使用 akshare 库获取上证指数的历史数据，并使用 statsmodels 库进行时间序列分析。主要包括： - 在线获取上证指数的历史数据，包括：收盘价、开盘价、最高价、最低价、成交量等。 - 计算日收益率、周收益率和年化收益率，并采用 matplotlib 库进行可视化。 - 图示收益率的分布特征 - 收益率的直方图、密度函数图等 - 收益率的自相关图、偏自相关图等 - 收益率的波动性分析 - 收益率标准差、方差等\n接下来，我们使用 Plotly 创建一个交互式折线图，用于展示上证指数在指定时间区间内的收盘价走势，并在鼠标悬停时显示对应的日期与日收益率等关键信息。相对于静态图形，交互式图形可以更好地展示数据的细节和趋势，便于用户进行深入分析。",
    "crumbs": [
      "金融数据分析",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>时间序列分析：上证指数的时序特征</span>"
    ]
  },
  {
    "objectID": "body/TS_SZ_index.html#交互图",
    "href": "body/TS_SZ_index.html#交互图",
    "title": "时间序列分析：上证指数的时序特征",
    "section": "",
    "text": "1. 数据准备\n\n时间筛选：通过 start_date 和 end_date 设置起始时间和结束时间，结合 pandas.to_datetime() 函数将日期列转换为标准时间格式，并筛选出指定时间区间内的数据。\n收益率合并：使用 merge() 函数将包含日收益率的 DataFrame 与主数据表按照日期列进行合并，使得每一日的收盘价配套显示对应的日收益率。\n\n\n\n2. 图形绘制\n\n图层添加：调用 go.Scatter() 添加一条收盘价的折线图（mode='lines' 表示仅显示折线，不显示节点）。\n交互信息：利用 hovertemplate 参数自定义鼠标悬停时显示的内容，包括：\n\n日期（格式化为 年-月-日）\n收盘价（保留两位小数）\n日收益率（百分号格式，保留两位小数）\n\n\n\n\n3. 图表布局设置\n\n通过 fig.update_layout() 设置图表标题、坐标轴标题、交互模式等：\n\nhovermode='x unified'：使得交互提示在同一垂直线上统一显示\ntemplate='plotly_white'：采用白色背景模板\n设置 margin 确保图形四周留有足够的空间，避免遮挡\n\n\n该图表可嵌入网页或 Jupyter Notebook 中动态展示，是教学、报告与展示金融时间序列数据的有力工具。\n\n#! pip install plotly\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom datetime import datetime\n\n# 设置起始时间和结束时间\nstart_date = '2019-01-01'\nend_date = datetime.today().strftime('%Y-%m-%d')\n\n# 将日期列转换为 datetime 类型（确保筛选条件生效）\nsz_index['day'] = pd.to_datetime(sz_index['day'])\nfiltered_sz_index['day'] = pd.to_datetime(filtered_sz_index['day'])\n\n# 筛选指定时间区间内的数据\nfiltered_data = sz_index.query(\" @start_date &lt;= day &lt;= @end_date \")\n\n# 合并 daily_return 信息（保持 hover 显示）\nfiltered_data = (\n    filtered_data\n    .merge(filtered_sz_index[['day', 'daily_return']], on='day', how='left')\n    .sort_values('day')  # 按日期排序，避免折线图错乱\n)\n\n# 创建交互式图形对象\nfig = go.Figure()\n\n# 添加收盘价的折线图\nfig.add_trace(go.Scatter(\n    x=filtered_data['day'],\n    y=filtered_data['close'],\n    mode='lines',  # 只显示线条\n    name='收盘价',\n    line=dict(color='blue'),\n    customdata=filtered_data[['daily_return']].values,\n    hovertemplate=(\n        '&lt;b&gt;日期：&lt;/b&gt; %{x|%Y-%m-%d}&lt;br&gt;'\n        '&lt;b&gt;收盘价：&lt;/b&gt; %{y:.2f}&lt;br&gt;'\n        '&lt;b&gt;日收益率：&lt;/b&gt; %{customdata[0]:.2%}&lt;extra&gt;&lt;/extra&gt;'\n    )\n))\n\n# 设置图表整体布局\nfig.update_layout(\n    title='上证指数交互图',\n    xaxis_title='日期',\n    yaxis_title='收盘价',\n    hovermode='x unified',\n    template='plotly_white',\n    margin=dict(l=60, r=40, t=60, b=50)\n)\n\n# 显示图表\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\n关键代码说明\n\n更统一的日期处理：\n\nsz_index['day'] = pd.to_datetime(...) 放在前面预处理，避免时间筛选出错。\n\n使用 .query() 简化筛选逻辑：\n\n更清晰的语法：query(\" @start_date &lt;= day &lt;= @end_date \")\n\n加入 .sort_values('day') 排序：\n\n防止合并后数据顺序错乱导致图形线条跳跃。\n\n更整洁的 hovertemplate：\n\n使用 %{x|%Y-%m-%d} 显示格式化日期，阅读体验更佳。\n\n注释和命名中文化，方便初学者理解和教学使用。",
    "crumbs": [
      "金融数据分析",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>时间序列分析：上证指数的时序特征</span>"
    ]
  },
  {
    "objectID": "body/TS_SZ_index.html#收益率",
    "href": "body/TS_SZ_index.html#收益率",
    "title": "时间序列分析：上证指数的时序特征",
    "section": "收益率",
    "text": "收益率\n收益率是金融时间序列分析中的重要指标，通常用于衡量资产价格变动的幅度和速度。我们将计算上证指数的日收益率、周收益率和年化收益率，并进行可视化展示。 - 日收益率：表示某一天的收盘价与前一天收盘价的比值变化，通常用百分比表示。 - 周收益率：表示某一周的收盘价与前一周收盘价的比值变化，通常用百分比表示。 - 年化收益率：表示某一年内的收益率，通常用百分比表示。年化收益率可以通过将日收益率乘以交易天数来计算。\n\n日收益率特征\n\n计算方法：日收益率 = (今日收盘价 - 昨日收盘价) / 昨日收盘价\n可视化：\n\n使用 plt.plot(x, y) 绘制日收益率的折线图，观察其波动趋势。\n使用 matplotlib 绘制日收益率的直方图和密度函数图，观察其分布特征。\n\n\n\n# 计算日收益率\nsz_index['daily_return'] = sz_index['close'].pct_change()  # 计算日收益率\n\n# 绘制日收益率走势图\n#-- 控制绘图的时间范围\n#start_plot_date = '1991-01-01'\nstart_plot_date = '2019-01-01'\nend_plot_date   = '2025-05-01'\nsz_index_plot = sz_index.query(\" @start_date &lt;= day &lt;= @end_date \")\n\n# 绘制日收益率走势图\nplt.figure(figsize=(16, 8))  # 调整图形尺寸\nplt.subplot(2, 1, 2)\nplt.plot(sz_index_plot['day'], \n         sz_index_plot['daily_return'], \n         label='Daily Return', \n         color='red')\nplt.title('Shanghai Composite Index Daily Return')\nplt.xlabel('Date')\nplt.ylabel('Daily Return')\nplt.legend()\nplt.grid()\nplt.tight_layout()  # 调整子图间距",
    "crumbs": [
      "金融数据分析",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>时间序列分析：上证指数的时序特征</span>"
    ]
  },
  {
    "objectID": "body/TS_SZ_index.html#收益率的直方图",
    "href": "body/TS_SZ_index.html#收益率的直方图",
    "title": "时间序列分析：上证指数的时序特征",
    "section": "收益率的直方图",
    "text": "收益率的直方图\n\n1. 直方图的基本原理\n直方图（Histogram）是一种用于展示数值型变量分布情况的图形工具。其原理是将数据划分为若干连续、不重叠的区间（称为“bin”或“箱子”），统计每个区间内数据点的数量，并以矩形的高度表示频数或频率。\n设有一组日收益率数据 \\(\\{r_1, r_2, \\ldots, r_n\\}\\)，我们将其划分为 \\(K\\) 个等宽的区间，每个区间的宽度为：\n\\[\nh = \\frac{\\max(r) - \\min(r)}{K}\n\\]\n第 \\(k\\) 个区间为 \\([a_k, a_{k+1})\\)，其频数记为 \\(f_k\\)，那么对应的矩形高度就是 \\(f_k\\)（或标准化后的频率）。绘图过程中，横轴表示收益率区间，纵轴表示该区间的频数或频率。\n\n\n2. 核心代码说明\n以下代码用于绘制上证指数的日收益率直方图：\nsz_index_plot['daily_return'].dropna().hist(bins=200, figsize=(8, 5))\n说明如下：\n\ndaily_return：表示日收益率列。\ndropna()：删除缺失值，避免影响绘图。\nhist()：调用 pandas.DataFrame.hist() 方法，底层封装了 matplotlib.pyplot.hist()。\nbins=200：将数据划分为 200 个等宽区间，越大越平滑，但过大可能导致过度拟合。\n\n\n\n3. 常用参数汇总\n\n\n\n\n\n\n\n\n参数名\n说明\n示例\n\n\n\n\nbins\n设置箱子的数量或箱边界\nbins=50，或 bins=[-0.1, -0.05, 0, 0.05, 0.1]\n\n\ndensity\n是否标准化为概率密度（面积为 1）\ndensity=True\n\n\nfigsize\n图形大小（宽, 高）\nfigsize=(10, 6)\n\n\ncolor\n设置柱体颜色\ncolor='skyblue'\n\n\nalpha\n设置透明度（0~1）\nalpha=0.7\n\n\ngrid\n是否显示网格\ngrid=True\n\n\n\n\n\n4. 实用建议\n\n若要观察收益率的分布是否对称，建议加上垂直参考线，例如均值或中位数。\n若需与正态分布对比，可叠加核密度曲线（使用 seaborn.histplot 或 sns.kdeplot）。\n若数据包含极端值，可调整 xlim 参数限制横轴范围，聚焦主要密度区域。\n\n直方图有助于识别收益率分布的偏态、厚尾特征，是金融时间序列分析中不可或缺的工具之一。\n\n# 绘制日收益率的基本直方图\nsz_index_plot['daily_return'].dropna().hist(bins=100,  # 设置直方图的柱子数量\n                                            color='green', \n                                            alpha=0.7, # 设置透明度\n                                            figsize=(8, 3))\n\n\n\n\n\n\n\n\n\n\n图形美化\n\n设置标题和坐标轴标签：使用 plt.title() 和 plt.xlabel()、plt.ylabel() 设置图形的标题和坐标轴标签，便于读者理解图形内容。\nx 轴范围：使用 plt.xlim() 设置 x 轴的范围，仅呈现主要数据区间，避免过多的空白区域影响阅读体验。\n添加注释：使用 plt.text() 在图形上添加注释，说明数据的时间范围。\n\n\n# 设置中文字体支持（如已设置可省略）\nplt.rcParams['font.sans-serif'] = ['SimHei'] \nplt.rcParams['axes.unicode_minus'] = False    \n\n# 创建图像，保持风格一致\nplt.figure(figsize=(8, 3))\nplt.hist(sz_index_plot['daily_return'].dropna(), \n         bins=100, \n         color='green', \n         alpha=0.7)\n\n# 设置标题和坐标轴\nplt.title('上证指数日收益率分布（截取区间）')\nplt.xlabel('日收益率')\nplt.ylabel('频数')\nplt.xlim(-0.03, 0.03)\nplt.grid(color='lightgray', linestyle='--', linewidth=0.5)\n\n# 添加注释文字（适度放大字号，突出展示）\nplt.text(0.03, plt.ylim()[1] * 0.9,\n         f\"样本时间区间: {start_plot_date} 至 {end_plot_date}\\n仅显示 |日收益率| &lt; 3% 的部分\",\n         fontsize=10, color='blue', ha='right', va='top')\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "金融数据分析",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>时间序列分析：上证指数的时序特征</span>"
    ]
  },
  {
    "objectID": "body/TS_SZ_index.html#核密度函数估计kernel-density-estimation-kde",
    "href": "body/TS_SZ_index.html#核密度函数估计kernel-density-estimation-kde",
    "title": "时间序列分析：上证指数的时序特征",
    "section": "核密度函数估计（Kernel Density Estimation, KDE）",
    "text": "核密度函数估计（Kernel Density Estimation, KDE）\n核密度估计是一种用于估计未知概率密度函数的非参数方法，适用于连续型数据且不依赖于事先指定的分布形式。其基本思想是：在密度函数的每一个估计点上，根据样本点到该点的距离，使用核函数分配权重并加权平均，从而构建平滑的密度曲线。\n设样本为 \\(x_1, x_2, \\dots, x_n\\)，其密度函数在任意点 \\(x\\) 上的估计形式为：\n\\[\n\\hat{f}_h(x) = \\frac{1}{n h} \\sum_{i=1}^{n} K\\left( \\frac{x - x_i}{h} \\right)\n\\]\n其中：\n\n\\(K(\\cdot)\\) 是核函数（kernel function），通常是一个对称的概率密度函数；\n\\(h &gt; 0\\) 是带宽参数（bandwidth），控制核函数的缩放程度和平滑水平；\n\\(\\hat{f}_h(x)\\) 是点 \\(x\\) 处的密度估计值。\n\n\n核函数\n在实际应用中，核函数的选择对估计结果的影响相对较小，而带宽的设置对估计曲线的光滑程度影响较大。\n核函数的作用可以理解为：在估计点 \\(x\\) 处，根据样本点 \\(x_i\\) 与 \\(x\\) 之间的距离，赋予不同的权重。距离 \\(x\\) 越近的样本点，其权重越大；距离越远，权重越小。通过对所有样本点的加权平均，得到该点的密度估计。将所有位置的估计值拼接起来，即可得到整体的密度函数曲线。\n为了更清楚地理解核函数的加权机制，我们可以对距离进行标准化处理，设：\n\\[\nu_i = \\frac{X_i - c}{h}\n\\]\n则以下两式等价：\n\\[\n|u_i| \\leq 1 \\Longleftrightarrow |X_i - c| \\leq h\n\\]\n记 \\(D_i = |X_i - c|\\)，表示第 \\(i\\) 个观察值与估计点 \\(c\\) 的距离。核函数的任务就是为每个 \\(D_i\\) 分配权重。\n如下图所示，三种典型核函数的权重分配机制具有显著差异：\n\n\nUniform 核：在 \\(|u| \\leq 1\\) 范围内赋予所有观察值相同的权重，超出范围的样本点权重为 0 (相当于弃之不用)。对应的密度估计不具有平滑性，常用于教学演示。\nTriangle 核：采用线性下降的加权方式，距离估计点越近权重越大，边界处权重为 0，估计结果具有一定的连续性。\nEpanechnikov 核：采用抛物线型权重函数，在 \\(u=0\\) 处取得最大值，具有最小均方误差（MSE）性质，估计曲线光滑、效率较高。\nGaussian 核：采用正态分布函数，所有样本点均有非零权重，平滑程度高，适用于大多数实际应用场景。\n\n\n\n核函数的性质\n常见核函数及其表达式：\n\nUniform 核函数 \\(K(u) = \\frac{1}{2} \\cdot \\mathbf{1}\\{\\left|u\\right| \\leq 1\\}\\) （也称为 Rectangular 核函数）\nTriangle 核函数 \\(K(u) = (1 - \\left|u\\right|) \\cdot \\mathbf{1}\\{\\left|u\\right| \\leq 1\\}\\)\nEpanechnikov 核函数 \\(K(u) = \\frac{3}{4}(1 - u^2) \\cdot \\mathbf{1}\\{\\left|u\\right| \\leq 1\\}\\)\nQuartic 核函数 \\(K(u) = \\frac{15}{16}(1 - u^2)^2 \\cdot \\mathbf{1}\\{\\left|u\\right| \\leq 1\\}\\)\nTriweight 核函数 \\(K(u) = \\frac{35}{32}(1 - u^2)^3 \\cdot \\mathbf{1}\\{\\left|u\\right| \\leq 1\\}\\)\nGaussian 核函数 \\(K(u) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{u^2}{2}\\right)\\)\nCosinus 核函数 \\(K(u) = \\frac{\\pi}{4} \\cos\\left(\\frac{\\pi}{2} u\\right) \\cdot \\mathbf{1}\\{\\left|u\\right| \\leq 1\\}\\)\n\n\n核函数通常需要满足以下数学性质：\n\n非负性：\\(K(u) \\geq 0\\)\n单位积分：\\(\\int_{-\\infty}^{\\infty} K(u) \\, du = 1\\)\n对称性：\\(K(u) = K(-u)\\)\n有限的二阶矩：\\(\\int u^2 K(u) \\, du &lt; \\infty\\)\n\n实际使用中，还有一些细节需要注意。例如，部分文献或软件将 \\(\\mathbf{1}\\{|u| \\leq 1\\}\\) 写为 \\(\\mathbf{1}\\{|u| &lt; 1\\}\\)。对于连续变量，两者几乎没有区别；但若数据是离散型的（如整数型变量），则可能影响边界值是否被纳入计算。\n核密度估计的构造可以理解为：以每一个样本点为中心放置一个缩放后的核函数，然后在每一个估计位置 \\(x\\) 上，取所有样本点的核值加权平均。因此，它是一种基于样本加权“局部贡献”的整体平滑过程。\n总结而言：\n\n核函数定义了如何根据样本点与估计点之间的距离分配权重；\n带宽参数决定了每个样本点的影响范围；\n合理选择核函数和带宽参数是核密度估计中最关键的步骤；\n核密度估计为我们提供了一种平滑、灵活且无需模型假设的分布估计方法，广泛应用于经济学、金融学、机器学习等领域的探索性数据分析任务中。\n\n\nimport seaborn as sns\n\n# 日收益率的核密度函数图\nsz_index_plot['daily_return'].plot(kind='kde')  #内置函数\n\n\n\n\n\n\n\n\n\n'''提示词\n用最简单的命令同时呈现 2005, 2010, 2015, 2020 和 2024 年的收益率密度函数图\n'''\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# 筛选指定年份的数据\n#selected_years = [2005, 2010, 2015, 2020, 2024]\nselected_years = [2005, 2015, 2024]\nfiltered_data = sz_index[sz_index['day'].dt.year.isin(selected_years)]\n\n# 创建图形对象\nplt.figure(figsize=(10, 6))\n\n# 绘制每个年份的核密度估计图\nfor year in selected_years:\n    sns.kdeplot(\n        data=filtered_data[filtered_data['day'].dt.year == year]['daily_return'].dropna(),\n        label=f'{year}'\n    )\n\n# 添加标题和图例\nplt.title(f'各年度收益率密度函数图)', fontsize=14)\nplt.xlabel('日收益率', fontsize=12)\nplt.ylabel('密度', fontsize=12)\nplt.legend(title='年份')\nplt.grid()\nplt.show()",
    "crumbs": [
      "金融数据分析",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>时间序列分析：上证指数的时序特征</span>"
    ]
  },
  {
    "objectID": "body/TS_SZ_index.html#周收益率和月收益率",
    "href": "body/TS_SZ_index.html#周收益率和月收益率",
    "title": "时间序列分析：上证指数的时序特征",
    "section": "周收益率和月收益率",
    "text": "周收益率和月收益率\n\n周收益率：(本周收盘价 - 上周收盘价) / 上周收盘价\n月收益率：(本月收盘价 - 上月收盘价) / 上月收盘价\n\n\n# 计算每周的收益率\nsz_index['week'] = sz_index['day'].dt.isocalendar().week  # 提取周数\nsz_index['year_week'] = sz_index['day'].dt.year.astype(str) \\\n                        + '-w' \\\n                        + sz_index['week'].astype(str).str.zfill(2)  # 组合年份和周数，周数补零\n\n# 计算每周的收益率\nweekly_return = sz_index.groupby('year_week')['close'].apply(lambda x: (x.iloc[-1] - x.iloc[0]) / x.iloc[0])\nweekly_return = weekly_return.reset_index()  # 重置索引\nweekly_return.columns = ['Year_Week', 'Weekly_Return']  # 重命名列\n\n# 打印结果\nprint(weekly_return.sort_values(by='Year_Week').tail(10))  # 按时间顺序显示最后20周的收益率\n\n     Year_Week  Weekly_Return\n1729  2025-w09      -0.015455\n1730  2025-w10       0.016769\n1731  2025-w11       0.015863\n1732  2025-w12      -0.017891\n1733  2025-w13      -0.005555\n1734  2025-w14       0.001877\n1735  2025-w15       0.045744\n1736  2025-w16       0.004267\n1737  2025-w17       0.001102\n1738  2025-w18      -0.002854\n\n\n\n# 设置起始时间和结束时间\nstart_week = '2005-w01'\nend_week = '2028-w16'\n\n# 筛选指定时间范围内的数据\nfiltered_weekly_return = weekly_return[(weekly_return['Year_Week'] &gt;= start_week) & \n                                       (weekly_return['Year_Week'] &lt;= end_week)]\n\n# 绘制每周收益率走势图\nplt.figure(figsize=(14, 7))\nplt.plot(filtered_weekly_return['Year_Week'], filtered_weekly_return['Weekly_Return'], label='Weekly Return', color='green')\nplt.title('Shanghai Composite Index Weekly Return')\nplt.xlabel('Year')\nplt.ylabel('Weekly Return')\n\n# 修改 x 轴标签，仅显示年份\nyear_labels = [label.split('-')[0] if label.endswith('-w01') else '' for label in filtered_weekly_return['Year_Week']]\nplt.xticks(ticks=range(len(year_labels)), labels=year_labels, rotation=0)\n\n# 添加 y=0 的水平线\nplt.axhline(y=0, color='red', linestyle='--', linewidth=1)\n\n# 设置 grid，仅显示主要的 grid\nplt.grid(visible=True, which='major', linestyle='-', linewidth=0.3)\n\nplt.legend()\nplt.tight_layout()  # 调整子图间距\nplt.show()  # 显示图形\n\n\n\n\n\n\n\n\n\n核心代码解读：\n\nsz_index['day'].dt.isocalendar().week：获取日期的周数。具体而言，dt.isocalendar() 返回一个 DataFrame，其中包含 ISO 日历的年、周和星期几。我们只需要周数，因此使用 .week 来提取它。类似的，可以使用 .dt.isocalendar().month 来获取月份；用 .dt.isocalendar().year 来获取年份。\n\nsz_index['day'].dt.isocalendar().year：获取日期的年份。类似地，使用 .year 来提取年份。\n\nweekly_return = sz_index.groupby('year_week')['close'].apply(lambda x: (x.iloc[-1] - x.iloc[0]) / x.iloc[0])：对每个周进行分组，计算该周的收益率。具体而言，groupby('year_week') 将数据按年和周进行分组，然后使用 apply() 函数对每个组应用一个 lambda 函数。这个 lambda 函数计算该组的最后一个收盘价和第一个收盘价之间的收益率。\n\napply() 函数的用法：apply() 函数可以对 DataFrame 或 Series 的每一行或每一列应用一个函数。它可以用于数据转换、聚合和其他操作。在这里，我们使用 apply() 函数来计算每个组的收益率。apply() 函数的语法格式为： python       DataFrame.apply(func, axis=0, raw=False, result_type=None, args=(), **kwds)\n\nfunc：要应用的函数，可以是 Python 内置函数或自定义函数。\naxis：指定应用函数的轴，0 表示按列应用，1 表示按行应用。默认值为 0。\nraw：如果为 True，则传递原始数据而不是 Series 对象。默认值为 False。\n\nlambda x: (x.iloc[-1] - x.iloc[0]) / x.iloc[0]：这是一个匿名函数，用于计算每个组的收益率。x 是传递给 lambda 函数的参数，表示当前组的数据。x.iloc[-1] 表示该组的最后一个收盘价，x.iloc[0] 表示该组的第一个收盘价。通过计算这两个值之间的差值并除以第一个收盘价，我们得到了该组的收益率。\n\n\n\n# 列出周收益率绝对值大于 0.2 的周\nhigh_weekly_return = weekly_return[weekly_return['Weekly_Return'].abs() &gt; 0.2]\n\n# 按收益率排序\nhigh_weekly_return = high_weekly_return.sort_values(by='Weekly_Return', ascending=False)  \nprint('\\n' + '---'*5 + 'high weekly return' + '---'*5)\nprint(high_weekly_return)  # 打印结果\n\n# 按时间排序\nhigh_weekly_return = high_weekly_return.sort_values(by='Year_Week')  # 按时间排序\nprint('\\n' + '---'*5 + 'sorted by Year_Week' + '---'*5)\nprint(high_weekly_return)  # 打印结果\n\n\n---------------high weekly return---------------\n     Year_Week  Weekly_Return\n72    1992-w21       1.343377\n0     1991-w01       1.272198\n259   1996-w01       0.704906\n99    1992-w48       0.553815\n1159  2014-w01       0.533468\n186   1994-w31       0.532717\n226   1995-w20       0.470363\n118   1993-w14       0.311382\n309   1997-w01       0.298735\n1415  2019-w01       0.237227\n507   2001-w01      -0.217497\n854   2008-w01      -0.654681\n\n---------------sorted by Year_Week---------------\n     Year_Week  Weekly_Return\n0     1991-w01       1.272198\n72    1992-w21       1.343377\n99    1992-w48       0.553815\n118   1993-w14       0.311382\n186   1994-w31       0.532717\n226   1995-w20       0.470363\n259   1996-w01       0.704906\n309   1997-w01       0.298735\n507   2001-w01      -0.217497\n854   2008-w01      -0.654681\n1159  2014-w01       0.533468\n1415  2019-w01       0.237227",
    "crumbs": [
      "金融数据分析",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>时间序列分析：上证指数的时序特征</span>"
    ]
  },
  {
    "objectID": "body/TS_SZ_index.html#年化收益率和标准差",
    "href": "body/TS_SZ_index.html#年化收益率和标准差",
    "title": "时间序列分析：上证指数的时序特征",
    "section": "年化收益率和标准差",
    "text": "年化收益率和标准差\n\n\n# 各个年度的收益率和标准差\nsz_index['year'] = sz_index['day'].dt.year  # 提取年份\nannual_stats = sz_index.groupby('year').agg({'daily_return': ['mean', 'std']}).reset_index()\n\n# 重命名列名\nannual_stats.columns = ['Year', 'Mean Daily Return', 'Std Daily Return']\n# 将收益率和标准差转换为百分比，并计算年化收益率和年化标准差\nannual_stats['Mean Daily Return'] = annual_stats['Mean Daily Return'] * 100  # 转换为百分比\nannual_stats['Std Daily Return'] = annual_stats['Std Daily Return'] * 100  # 转换为百分比\nannual_stats['Annualized Return'] = annual_stats['Mean Daily Return'] * 252 / 100  # 年化收益率\nannual_stats['Annualized Std'] = annual_stats['Std Daily Return'] * (252 ** 0.5) / 100  # 年化标准差\n# 打印单数年份的收益率和标准差，小数点后保留三位，四列在同一行呈现\nprint(annual_stats[annual_stats['Year'] % 2 == 1][['Year', 'Mean Daily Return', 'Std Daily Return', 'Annualized Return', 'Annualized Std']].round(3).to_string(index=False))\n\n Year  Mean Daily Return  Std Daily Return  Annualized Return  Annualized Std\n 1991              0.326             0.662              0.821           0.105\n 1993              0.096             3.781              0.243           0.600\n 1995             -0.016             3.103             -0.041           0.493\n 1997              0.133             2.194              0.335           0.348\n 1999              0.089             1.773              0.224           0.281\n 2001             -0.087             1.387             -0.218           0.220\n 2003              0.047             1.143              0.118           0.181\n 2005             -0.027             1.378             -0.067           0.219\n 2007              0.305             2.216              0.768           0.352\n 2009              0.259             1.901              0.653           0.302\n 2011             -0.093             1.156             -0.235           0.183\n 2013             -0.023             1.159             -0.057           0.184\n 2015              0.067             2.447              0.169           0.388\n 2017              0.028             0.547              0.069           0.087\n 2019              0.089             1.140              0.224           0.181\n 2021              0.023             0.881              0.058           0.140\n 2023             -0.013             0.729             -0.033           0.116\n 2025             -0.021             1.152             -0.054           0.183\n\n\n\n# 图示各个年度收益率和标准差\nplt.figure(figsize=(14, 7))\nplt.subplot(2, 1, 1)\nplt.bar(annual_stats['Year'], annual_stats['Mean Daily Return'], color='blue', label='Mean Daily Return')\nplt.title('Annual Mean Daily Return of Shanghai Composite Index')\nplt.xlabel('Year')\nplt.ylabel('Mean Daily Return')\nplt.legend()\nplt.grid()\nplt.subplot(2, 1, 2)\nplt.bar(annual_stats['Year'], annual_stats['Std Daily Return'], color='orange', label='Std Daily Return')\nplt.title('Annual Std Daily Return of Shanghai Composite Index')\nplt.xlabel('Year')\nplt.ylabel('Std Daily Return')\nplt.legend()\nplt.grid()\nplt.tight_layout()  # 调整子图间距\nplt.show()  # 显示图形\n\n\n\n\n\n\n\n\n\n\n# 图示波动率\nplt.figure(figsize=(14, 7))\nplt.subplot(2, 1, 1)\nplt.plot(sz_index['day'], \n         sz_index['daily_return'].rolling(window=30).std(), \n         label='30-Day Rolling Volatility', \n         color='red')\nplt.title('SZ Index Daily Return Volatility')\nplt.xlabel('Date')\nplt.ylabel('Volatility')\nplt.legend()\nplt.grid()\nplt.show()  # 显示图形\n\n\n\n\n\n\n\n\n\n\n# 将日期列转换为 datetime 类型\nsz_index['day'] = pd.to_datetime(sz_index['day'])\n\n# 将收盘价列转换为浮点数类型\nsz_index['close'] = sz_index['close'].astype('float')\n\n# 创建一个示例 DataFrame，用于合并\ndata = pd.DataFrame({'time': sz_index['day'], 'pos_p': [0] * len(sz_index)})\n\n# 合并两个 DataFrame，基于时间列进行内连接\ndata = data.merge(sz_index, left_on='time', right_on='day', how='inner')\n\n# 绘制图表\nplt.figure(figsize=(4, 3))  # 设置图表大小\ndata.index = data['time']  # 将时间列设置为索引\ndata[['pos_p', 'close']].plot(secondary_y=['close'])  # 绘制双 Y 轴图表\nplt.title('SH000001 15min K-line')  # 设置图表标题\nplt.show()  # 显示图表\n\n&lt;Figure size 400x300 with 0 Axes&gt;",
    "crumbs": [
      "金融数据分析",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>时间序列分析：上证指数的时序特征</span>"
    ]
  }
]